{"config":{"lang":["en"],"separator":"[\\s\\-\\_\\.]","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GenAI Bench","text":"<p>Unified, accurate, and beautiful LLM Benchmarking</p> <p> </p>"},{"location":"#what-is-genai-bench","title":"What is GenAI Bench?","text":"<p>Genai-bench is a powerful benchmark tool designed for comprehensive token-level performance evaluation of large language model (LLM) serving systems.</p> <p>It provides detailed insights into model serving performance, offering both a user-friendly CLI and a live UI for real-time progress monitoring.</p>"},{"location":"#live-ui-dashboard","title":"Live UI Dashboard","text":"<p>GenAI Bench includes a real-time dashboard that provides live monitoring of your benchmarks:</p> <p></p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83d\udee0\ufe0f CLI Tool: Validates user inputs and initiates benchmarks seamlessly.</li> <li>\ud83d\udcca Live UI Dashboard: Displays current progress, logs, and real-time metrics.</li> <li>\ud83d\udcdd Rich Logs: Automatically flushed to both terminal and file upon experiment completion.</li> <li>\ud83d\udcc8 Experiment Analyzer: Generates comprehensive Excel reports with pricing and raw metrics data, plus flexible plot configurations (default 2x4 grid) that visualize key performance metrics including throughput, latency (TTFT, E2E, TPOT), error rates, and RPS across different traffic scenarios and concurrency levels. Supports custom plot layouts and multi-line comparisons.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Get started with GenAI Bench in minutes:</p> <pre><code># Install from PyPI\npip install genai-bench\n\n# Run your first benchmark\ngenai-bench benchmark --help\n</code></pre> <p>For detailed installation and usage instructions, see our Installation Guide.</p>"},{"location":"#supported-tasks","title":"Supported Tasks","text":"<p>GenAI Bench supports multiple benchmark types:</p> Task Description Use Case <code>text-to-text</code> Benchmarks generating text output from text input Chat, QA <code>text-to-embeddings</code> Benchmarks generating embeddings from text input Semantic search <code>image-text-to-text</code> Benchmarks generating text from images and text prompts Visual question answering <code>image-to-embeddings</code> Benchmarks generating embeddings from images Image similarity"},{"location":"#documentation-sections","title":"Documentation Sections","text":""},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<ul> <li>Installation - Detailed installation guide</li> <li>Task Definition - Understanding different benchmark tasks</li> <li>Command Guidelines - Command usage guidelines</li> <li>Metrics Definition - Understanding benchmark metrics</li> </ul>"},{"location":"#user-guide","title":"\ud83d\udcd6 User Guide","text":"<ul> <li>Run Benchmark - How to run benchmarks</li> <li>Multi-Cloud Authentication &amp; Storage - Comprehensive guide for cloud provider authentication</li> <li>Multi-Cloud Quick Reference - Quick examples for common scenarios</li> <li>Run Benchmark with Docker - Docker-based benchmarking</li> <li>Generate Excel Sheet - Creating Excel reports</li> <li>Generate Plot - Creating visualizations</li> <li>Upload Benchmark Results - Uploading results</li> </ul>"},{"location":"#development","title":"\ud83d\udd27 Development","text":"<ul> <li>Contributing - How to contribute to GenAI Bench</li> </ul>"},{"location":"#support","title":"Support","text":"<p>If you encounter any issues or have questions, please: - Check our documentation for detailed guides - Report issues on our GitHub repository - Join our community discussions</p>"},{"location":"#license","title":"License","text":"<p>GenAI Bench is open source and available under the MIT License. </p>"},{"location":"api/","title":"API Reference","text":"<p>This section provides detailed API documentation for GenAI Bench components.</p> <p>Coming Soon</p> <p>Comprehensive API documentation is being developed. In the meantime, please refer to the source code docstrings.</p>"},{"location":"api/#core-components","title":"Core Components","text":""},{"location":"api/#authentication","title":"Authentication","text":"<ul> <li>UnifiedAuthFactory - Factory for creating authentication providers</li> <li>ModelAuthProvider - Base class for model authentication</li> <li>StorageAuthProvider - Base class for storage authentication</li> </ul>"},{"location":"api/#storage","title":"Storage","text":"<ul> <li>BaseStorage - Abstract base class for storage implementations</li> <li>StorageFactory - Factory for creating storage providers</li> </ul>"},{"location":"api/#cli","title":"CLI","text":"<ul> <li>option_groups - Modular CLI option definitions</li> <li>validation - Input validation functions</li> </ul>"},{"location":"api/#metrics","title":"Metrics","text":"<ul> <li>AggregatedMetricsCollector - Collects and aggregates benchmark metrics</li> <li>RequestMetricsCollector - Collects per-request metrics</li> </ul>"},{"location":"api/#user-classes","title":"User Classes","text":"<ul> <li>BaseUser - Abstract base class for user implementations</li> <li>OpenAIUser - OpenAI API implementation</li> <li>AWSBedrockUser - AWS Bedrock implementation</li> <li>AzureOpenAIUser - Azure OpenAI implementation</li> <li>GCPVertexUser - GCP Vertex AI implementation</li> <li>OCICohereUser - OCI Cohere implementation</li> </ul>"},{"location":"api/#example-usage","title":"Example Usage","text":""},{"location":"api/#creating-an-authentication-provider","title":"Creating an Authentication Provider","text":"<pre><code>from genai_bench.auth.unified_factory import UnifiedAuthFactory\n\n# Create OpenAI auth\nauth = UnifiedAuthFactory.create_model_auth(\n    \"openai\",\n    api_key=\"sk-...\"\n)\n\n# Create AWS Bedrock auth\nauth = UnifiedAuthFactory.create_model_auth(\n    \"aws-bedrock\",\n    access_key_id=\"AKIA...\",\n    secret_access_key=\"...\",\n    region=\"us-east-1\"\n)\n</code></pre>"},{"location":"api/#creating-a-storage-provider","title":"Creating a Storage Provider","text":"<pre><code>from genai_bench.auth.unified_factory import UnifiedAuthFactory\nfrom genai_bench.storage.factory import StorageFactory\n\n# Create storage auth\nstorage_auth = UnifiedAuthFactory.create_storage_auth(\n    \"aws\",\n    profile=\"default\",\n    region=\"us-east-1\"\n)\n\n# Create storage instance\nstorage = StorageFactory.create_storage(\n    \"aws\",\n    storage_auth\n)\n\n# Upload a folder\nstorage.upload_folder(\n    \"/path/to/results\",\n    \"my-bucket\",\n    prefix=\"benchmarks/2024\"\n)\n</code></pre>"},{"location":"api/#contributing-to-api-documentation","title":"Contributing to API Documentation","text":"<p>We welcome contributions to improve our API documentation! If you'd like to help:</p> <ol> <li>Add docstrings to undocumented functions</li> <li>Provide usage examples</li> <li>Document edge cases and gotchas</li> <li>Submit a pull request</li> </ol> <p>See our Contributing Guide for more details.</p>"},{"location":"development/","title":"Development","text":"<p>Welcome to the GenAI Bench development guide! This section covers everything you need to contribute to the project.</p>"},{"location":"development/#getting-started-with-development","title":"Getting Started with Development","text":"<ul> <li> <p> Contributing</p> <p>Learn how to contribute to GenAI Bench</p> <p> Contributing Guide</p> </li> </ul>"},{"location":"development/#development-setup","title":"Development Setup","text":""},{"location":"development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>Git</li> <li>Make (optional but recommended)</li> </ul>"},{"location":"development/#clone-the-repository","title":"Clone the Repository","text":"<pre><code>git clone https://github.com/sgl-project/genai-bench.git\ncd genai-bench\n</code></pre>"},{"location":"development/#create-a-virtual-environment","title":"Create a Virtual Environment","text":"<pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre>"},{"location":"development/#install-in-development-mode","title":"Install in Development Mode","text":"<pre><code>pip install -e \".[dev]\"\n</code></pre>"},{"location":"development/#run-tests","title":"Run Tests","text":"<pre><code>make test\n</code></pre>"},{"location":"development/#code-formatting","title":"Code Formatting","text":"<pre><code>make format\n</code></pre>"},{"location":"development/#linting","title":"Linting","text":"<pre><code>make lint\n</code></pre>"},{"location":"development/#project-structure","title":"Project Structure","text":"<pre><code>genai-bench/\n\u251c\u2500\u2500 genai_bench/          # Main package\n\u2502   \u251c\u2500\u2500 auth/            # Authentication providers\n\u2502   \u251c\u2500\u2500 cli/             # CLI implementation\n\u2502   \u251c\u2500\u2500 metrics/         # Metrics collection\n\u2502   \u251c\u2500\u2500 storage/         # Storage providers\n\u2502   \u2514\u2500\u2500 user/            # User implementations\n\u251c\u2500\u2500 tests/               # Test suite\n\u251c\u2500\u2500 docs/                # Documentation\n\u2514\u2500\u2500 examples/            # Example configurations\n</code></pre>"},{"location":"development/#key-components","title":"Key Components","text":""},{"location":"development/#authentication-system","title":"Authentication System","text":"<ul> <li>Unified factory for creating auth providers</li> <li>Support for multiple cloud providers</li> <li>Extensible architecture for new providers</li> </ul>"},{"location":"development/#storage-system","title":"Storage System","text":"<ul> <li>Abstract base class for storage providers</li> <li>Implementations for AWS S3, Azure Blob, GCP Cloud Storage, etc.</li> <li>Consistent interface across providers</li> </ul>"},{"location":"development/#cli-architecture","title":"CLI Architecture","text":"<ul> <li>Click-based command structure</li> <li>Modular option groups</li> <li>Comprehensive validation</li> </ul>"},{"location":"development/#adding-new-features","title":"Adding New Features","text":""},{"location":"development/#adding-a-new-model-provider","title":"Adding a New Model Provider","text":"<ol> <li>Create auth provider in <code>genai_bench/auth/</code></li> <li>Create user class in <code>genai_bench/user/</code></li> <li>Update <code>UnifiedAuthFactory</code></li> <li>Add validation in <code>cli/validation.py</code></li> <li>Write tests</li> </ol>"},{"location":"development/#adding-a-new-storage-provider","title":"Adding a New Storage Provider","text":"<ol> <li>Create storage auth in <code>genai_bench/auth/</code></li> <li>Create storage implementation in <code>genai_bench/storage/</code></li> <li>Update <code>StorageFactory</code></li> <li>Write tests</li> </ol>"},{"location":"development/#testing","title":"Testing","text":"<p>We use pytest for testing:</p> <pre><code># Run all tests\npytest\n\n# Run specific test file\npytest tests/auth/test_unified_factory.py\n\n# Run with coverage\npytest --cov=genai_bench\n\n# Run specific test\npytest -k \"test_openai_auth\"\n</code></pre>"},{"location":"development/#documentation","title":"Documentation","text":"<p>Documentation uses MkDocs Material:</p> <pre><code># Install docs dependencies\npip install mkdocs-material\n\n# Serve docs locally\nmkdocs serve\n\n# Build docs\nmkdocs build\n</code></pre>"},{"location":"development/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8</li> <li>Use type hints</li> <li>Write docstrings for public APIs</li> <li>Keep functions focused and small</li> <li>Add tests for new features</li> </ul>"},{"location":"development/#questions","title":"Questions?","text":"<ul> <li>Open an issue on GitHub</li> <li>Join our community discussions</li> <li>Check existing issues and PRs</li> </ul>"},{"location":"development/contributing/","title":"Contribution Guideline","text":"<p>Welcome and thank you for your interest in contributing to genai-bench.</p>"},{"location":"development/contributing/#coding-style-guide","title":"Coding Style Guide","text":"<p>genai-bench uses python 3.11, and we adhere to Google Python style guide.</p> <p>We use <code>make format</code> to format our code using <code>isort</code> and <code>ruff</code>. The detailed configuration can be found in pyproject.toml.</p>"},{"location":"development/contributing/#pull-requests","title":"Pull Requests","text":"<p>When submitting a pull request, please:</p> <ol> <li><code>git pull origin main</code> to make sure your code has been rebased on top of the latest commit on the main branch.</li> <li>Ensure code is properly formatted and passed every lint checks by running <code>make check</code>.</li> <li>Add new test cases to stay robust and correct. In the case of a bug fix, the tests should fail without your code changes. For new features, try to cover as many variants as reasonably possible. You can use <code>make test</code> to check the test coverage. Or use <code>make test_changed</code> to test the test coverage for your own branch. We enforce to keep a test coverage about 90%.</li> </ol>"},{"location":"development/contributing/#pr-template","title":"PR Template","text":"<p>It is required to classify your PR and make the commit message concise and useful. Prefix the PR title appropriately to indicate the type of change. Please use one of the following:</p> <p><code>[Bugfix]</code> for bug fixes.</p> <p><code>[Core]</code> for core backend changes. This includes build, version upgrade, changes in user and sampling.</p> <p><code>[Metrics]</code> for changes made to metrics.</p> <p><code>[Frontend]</code> for UI dashboard and CLI entrypoint related changes.</p> <p><code>[Docs]</code> for changes related to documentation.</p> <p><code>[CI/Tests]</code> for unittests and integration tests.</p> <p><code>[Report]</code> for changes in generating plots and excel reports.</p> <p><code>[Misc]</code> for PRs that do not fit the above categories. Please use this sparingly.</p> <p>Open source community also recommends to keep the commit message title within 52 chars and each line in message content within 72 chars.</p>"},{"location":"development/contributing/#code-reviews","title":"Code Reviews","text":"<p>All submissions, including submissions by project members, require a code review. To make the review process as smooth as possible, please:</p> <ol> <li>Keep your changes as concise as possible.    If your pull request involves multiple unrelated changes, consider splitting it into separate pull requests.</li> <li>Respond to all comments within a reasonable time frame.    If a comment isn't clear,    or you disagree with a suggestion, feel free to ask for clarification or discuss the suggestion.</li> <li>Provide constructive feedback and meaningful comments. Focus on specific improvements    and suggestions that can enhance the code quality or functionality. Remember to    acknowledge and respect the work the author has already put into the submission.</li> </ol>"},{"location":"development/contributing/#setup-development-environment","title":"Setup Development Environment","text":""},{"location":"development/contributing/#make","title":"<code>make</code>","text":"<p>genai-bench utilizes <code>make</code> for a lot of useful commands.</p> <p>If your laptop doesn't have <code>GNU make</code> installed, (check this by typing <code>make --version</code> in your terminal), you can ask our GenerativeAI's chatbot about how to install it in your system.</p>"},{"location":"development/contributing/#uv","title":"<code>uv</code>","text":"<p>Install uv with <code>make uv</code> or install it from the official website. If installing from the website, create a project venv with <code>uv venv -p python3.11</code>.</p> <p>Once you have <code>make</code> and <code>uv</code> installed, you can follow the command below to build genai-bench wheel:</p> <pre><code># check out commands genai-bench supports\nmake help\n#activate virtual env managed by uv\nsource .venv/bin/activate\n# install dependencies\nmake install\n</code></pre> <p>You can utilize wheel to install genai-bench.</p> <pre><code># build a .whl under genai-bench/dist\nmake build\n# send the wheel to your remote machine if applies\nrsync --delete -avz ~/genai-bench/dist/&lt;.wheel&gt; &lt;remote-user&gt;@&lt;remote-ip&gt;:&lt;dest-addr&gt;\n</code></pre> <p>On your remote machine, you can simply use the <code>pip</code> to install genai-bench.</p> <pre><code>pip install &lt;dest-addr&gt;/&lt;.wheel&gt;\n</code></pre>"},{"location":"development/contributing/#development-guide-adding-a-new-task-in-genai-bench","title":"Development Guide: Adding a New Task in <code>genai-bench</code>","text":"<p>This guide explains how to add support for a new task in <code>genai-bench</code>. Follow the steps below to ensure consistency and compatibility with the existing codebase.</p>"},{"location":"development/contributing/#1-define-the-request-and-response-in-protocolpy","title":"1. Define the Request and Response in <code>protocol.py</code>","text":""},{"location":"development/contributing/#steps","title":"Steps","text":"<ol> <li>Add relevant fields to the appropriate request/response data classes in <code>protocol.py</code></li> <li>If the new task involves a new input-output modality, create a new request/response class.</li> <li>Use existing request/response classes (<code>UserChatRequest</code>, <code>UserEmbeddingRequest</code>, <code>UserImageChatRequest</code>, etc.) if they suffice.</li> </ol>"},{"location":"development/contributing/#example","title":"Example","text":"<pre><code>class UserTextToImageRequest(UserRequest):\n    \"\"\"Represents a request for generating images from text.\"\"\"\n    prompt: str\n    num_images: int = Field(..., description=\"Number of images to generate.\")\n    image_resolution: Tuple[int, int] = Field(..., description=\"Resolution of the generated images.\")\n</code></pre>"},{"location":"development/contributing/#2-update-or-create-a-sampler","title":"2. Update or Create a Sampler","text":""},{"location":"development/contributing/#21-if-input-modality-is-supported-by-an-existing-sampler","title":"2.1 If Input Modality Is Supported by an Existing Sampler","text":"<ol> <li>Check if the current <code>TextSampler</code> or <code>ImageSampler</code> supports the input-modality.</li> <li>Add request creation logic in the relevant <code>TextSampler</code> or <code>ImageSampler</code> class.</li> <li>Refactor the sampler's <code>_create_request</code> method to support the new task.</li> <li>Tip: Avoid adding long <code>if-else</code> chains for new tasks. Utilize helper methods or design a request creator pattern if needed.</li> </ol>"},{"location":"development/contributing/#22-if-input-modality-is-not-supported","title":"2.2 If Input Modality Is Not Supported","text":"<ol> <li>Create a new sampler class inheriting from <code>BaseSampler</code>.</li> <li>Define the <code>sample</code> method to generate requests for the new task.</li> <li>Refer to <code>TextSampler</code> and <code>ImageSampler</code> for implementation patterns.</li> <li>Add utility functions for data preprocessing or validation specific to the new modality if necessary.</li> </ol>"},{"location":"development/contributing/#example-for-a-new-sampler","title":"Example for a New Sampler","text":"<pre><code>class AudioSampler(Sampler):\n    input_modality = \"audio\"\n    supported_tasks = {\"audio-to-text\", \"audio-to-embeddings\"}\n\n    def sample(self, scenario: Scenario) -&gt; UserRequest:\n        # Validate scenario\n        self._validate_scenario(scenario)\n\n        if self.output_modality == \"text\":\n            return self._create_audio_to_text_request(scenario)\n        elif self.output_modality == \"embeddings\":\n            return self._create_audio_to_embeddings_request(scenario)\n        else:\n            raise ValueError(f\"Unsupported output_modality: {self.output_modality}\")\n</code></pre>"},{"location":"development/contributing/#3-add-task-support-in-the-user-class","title":"3. Add Task Support in the User Class","text":"<p>Each <code>User</code> corresponds to one API backend, such as <code>OpenAIUser</code> for OpenAI. Users can have multiple tasks, each corresponding to an endpoint.</p>"},{"location":"development/contributing/#steps_1","title":"Steps","text":"<ol> <li>Add the new task to the <code>supported_tasks</code> dictionary in the relevant <code>User</code> class.</li> <li>Map the new task to its corresponding function name in the dictionary.</li> <li>Implement the new function in the <code>User</code> class for handling the task logic.</li> <li>If the new task uses an existing endpoint, refactor the function to support both tasks without duplicating logic.</li> <li>Important: Avoid creating multiple functions for tasks that use the same endpoint.</li> </ol>"},{"location":"development/contributing/#example_1","title":"Example","text":"<pre><code>class OpenAIUser(BaseUser):\n    supported_tasks = {\n        \"text-to-text\": \"chat\",\n        \"image-text-to-text\": \"chat\",\n        \"text-to-embeddings\": \"embeddings\",\n        \"audio-to-text\": \"audio_to_text\",  # New task added\n    }\n\n    def audio_to_text(self):\n        # Implement the logic for audio-to-text task\n        endpoint = \"/v1/audio/transcriptions\"\n        user_request = self.sample()\n\n        # Add payload and send request\n        payload = {\"audio\": user_request.audio_file}\n        self.send_request(False, endpoint, payload, self.parse_audio_response)\n</code></pre>"},{"location":"development/contributing/#4-add-unit-tests","title":"4. Add Unit Tests","text":""},{"location":"development/contributing/#steps_2","title":"Steps","text":"<ol> <li>Add tests for the new task in the appropriate test files.</li> <li>Include tests for:</li> <li>Request creation in the sampler.</li> <li>Task validation in the <code>User</code> class.</li> <li>End-to-end workflow using the new task.</li> </ol>"},{"location":"development/contributing/#5-update-documentation","title":"5. Update Documentation","text":""},{"location":"development/contributing/#steps_3","title":"Steps","text":"<ol> <li>Add the new task to the list of supported tasks in the Task Definition guide.</li> <li>Provide sample commands and explain any required configuration changes.</li> <li>Mention the new task in this contributing guide for future developers.</li> </ol>"},{"location":"examples/","title":"Examples","text":"<p>This section provides practical examples and configurations for GenAI Bench.</p>"},{"location":"examples/#available-examples","title":"Available Examples","text":"<ul> <li> <p> Plot Configuration</p> <p>Learn how to customize visualization plots</p> <p> Plot Examples</p> </li> </ul>"},{"location":"examples/#quick-examples","title":"Quick Examples","text":""},{"location":"examples/#openai-gpt-4-benchmark","title":"OpenAI GPT-4 Benchmark","text":"<pre><code>genai-bench benchmark \\\n  --api-backend openai \\\n  --api-base https://api.openai.com/v1 \\\n  --api-key $OPENAI_API_KEY \\\n  --api-model-name gpt-4 \\\n  --model-tokenizer gpt2 \\\n  --task text-to-text \\\n  --max-requests-per-run 1000 \\\n  --max-time-per-run 10\n</code></pre>"},{"location":"examples/#aws-bedrock-claude-benchmark","title":"AWS Bedrock Claude Benchmark","text":"<pre><code>genai-bench benchmark \\\n  --api-backend aws-bedrock \\\n  --api-base https://bedrock-runtime.us-east-1.amazonaws.com \\\n  --aws-profile default \\\n  --aws-region us-east-1 \\\n  --api-model-name anthropic.claude-3-sonnet-20240229-v1:0 \\\n  --model-tokenizer Anthropic/claude-3-sonnet \\\n  --task text-to-text \\\n  --max-requests-per-run 500 \\\n  --max-time-per-run 10\n</code></pre>"},{"location":"examples/#multi-modal-benchmark","title":"Multi-Modal Benchmark","text":"<pre><code>genai-bench benchmark \\\n  --api-backend gcp-vertex \\\n  --api-base https://us-central1-aiplatform.googleapis.com \\\n  --gcp-project-id my-project \\\n  --gcp-location us-central1 \\\n  --gcp-credentials-path /path/to/service-account.json \\\n  --api-model-name gemini-1.5-pro-vision \\\n  --model-tokenizer google/gemini \\\n  --task image-text-to-text \\\n  --dataset-path /path/to/images \\\n  --max-requests-per-run 100 \\\n  --max-time-per-run 10\n</code></pre>"},{"location":"examples/#embedding-benchmark-with-batch-sizes","title":"Embedding Benchmark with Batch Sizes","text":"<pre><code>genai-bench benchmark \\\n  --api-backend openai \\\n  --api-base https://api.openai.com/v1 \\\n  --api-key $OPENAI_API_KEY \\\n  --api-model-name text-embedding-3-large \\\n  --model-tokenizer cl100k_base \\\n  --task text-to-embeddings \\\n  --batch-size 1 --batch-size 8 --batch-size 32 --batch-size 64 \\\n  --max-requests-per-run 2000 \\\n  --max-time-per-run 10\n</code></pre>"},{"location":"examples/#traffic-scenarios","title":"Traffic Scenarios","text":"<p>GenAI Bench supports various traffic patterns:</p>"},{"location":"examples/#text-generation-scenarios","title":"Text Generation Scenarios","text":"<ul> <li><code>D(100,100)</code> - Deterministic: 100 input tokens, 100 output tokens</li> <li><code>N(480,240)/(300,150)</code> - Normal distribution</li> <li><code>U(50,100)/(200,250)</code> - Uniform distribution</li> </ul>"},{"location":"examples/#embedding-scenarios","title":"Embedding Scenarios","text":"<ul> <li><code>E(64)</code> - 64 tokens per document</li> <li><code>E(512)</code> - 512 tokens per document</li> <li><code>E(1024)</code> - 1024 tokens per document</li> </ul>"},{"location":"examples/#vision-scenarios","title":"Vision Scenarios","text":"<ul> <li><code>I(512,512)</code> - 512x512 pixel images</li> <li><code>I(1024,512)</code> - 1024x512 pixel images</li> <li><code>I(2048,2048)</code> - 2048x2048 pixel images</li> </ul>"},{"location":"examples/#contributing-examples","title":"Contributing Examples","text":"<p>Have a useful configuration or example? We welcome contributions! Please submit a pull request with your example following our contribution guidelines.</p>"},{"location":"examples/plot-config-examples/","title":"Plot Configuration Examples","text":"<p>This directory contains example plot configurations for the flexible plotting system in genai-bench.</p>"},{"location":"examples/plot-config-examples/#usage","title":"Usage","text":"<p>Use these configurations with the <code>genai-bench plot</code> command:</p> <pre><code># Use a custom configuration file\ngenai-bench plot --experiments-folder /path/to/experiments \\\n                 --group-key traffic_scenario \\\n                 --plot-config examples/plot_configs/custom_2x2.json\n\n# Use a built-in preset for multiple scenarios\ngenai-bench plot --experiments-folder /path/to/experiments \\\n                 --group-key traffic_scenario \\\n                 --preset simple_2x2\n\n# Use multi-line preset for single scenario analysis\ngenai-bench plot --experiments-folder /path/to/experiments \\\n                 --group-key none \\\n                 --preset single_scenario_analysis\n\n# List available fields with actual data from your experiment\ngenai-bench plot --experiments-folder /path/to/experiments \\\n                 --group-key traffic_scenario \\\n                 --list-fields\n\n# Validate a configuration without generating plots\ngenai-bench plot --experiments-folder /path/to/experiments \\\n                 --group-key traffic_scenario \\\n                 --plot-config examples/plot_configs/custom_2x2.json \\\n                 --validate-only\n</code></pre>"},{"location":"examples/plot-config-examples/#available-configurations","title":"Available Configurations","text":""},{"location":"examples/plot-config-examples/#custom_2x2json","title":"custom_2x2.json","text":"<p>A simple 2x2 grid layout focusing on key performance metrics: - Throughput vs Mean Latency - RPS vs P99 Latency - Concurrency vs TTFT - Error Rate Analysis</p>"},{"location":"examples/plot-config-examples/#performance_focusedjson","title":"performance_focused.json","text":"<p>A comprehensive 2x3 grid for detailed performance analysis: - Token generation speed analysis - Time to first token trends - Latency percentiles - Token efficiency scatter plot - Request success rates - Throughput scaling</p>"},{"location":"examples/plot-config-examples/#multi_line_latencyjson","title":"multi_line_latency.json","text":"<p>Demonstrates multi-line plotting capabilities with a 2x2 layout: - Latency Percentiles Comparison: Multiple latency percentiles (mean, P90, P99) on one plot - TTFT Performance Analysis: Mean and P95 TTFT comparison - Token Processing Speed: Output speed vs input throughput comparison - Request Success Metrics: Single-line error rate plot</p>"},{"location":"examples/plot-config-examples/#comprehensive_multi_linejson","title":"comprehensive_multi_line.json","text":"<p>Advanced multi-line example with 1x3 layout showcasing complex comparisons: - E2E Latency Distribution: All percentiles (P25, P50, P75, P90, P99) with custom colors - Throughput Components: Input, output, and total throughput comparison - Token Statistics: Input, output, and total token counts as scatter plot</p>"},{"location":"examples/plot-config-examples/#configuration-format","title":"Configuration Format","text":"<p>Plot configurations use the following JSON schema:</p>"},{"location":"examples/plot-config-examples/#single-line-plots","title":"Single-Line Plots","text":"<pre><code>{\n  \"layout\": {\n    \"rows\": 2,\n    \"cols\": 2,\n    \"figsize\": [16, 12]  // Optional: [width, height] in inches\n  },\n  \"plots\": [\n    {\n      \"title\": \"Plot Title\",\n      \"x_field\": \"field.path.from.AggregatedMetrics\",\n      \"y_field\": \"another.field.path\",           // Single field\n      \"x_label\": \"Custom X Label\",              // Optional\n      \"y_label\": \"Custom Y Label\",              // Optional\n      \"plot_type\": \"line\",                      // line, scatter, or bar\n      \"position\": [0, 0]                        // [row, col] in grid\n    }\n  ]\n}\n</code></pre>"},{"location":"examples/plot-config-examples/#multi-line-plots","title":"Multi-Line Plots","text":"<pre><code>{\n  \"plots\": [\n    {\n      \"title\": \"Multi-Line Comparison\",\n      \"x_field\": \"requests_per_second\",\n      \"y_fields\": [                             // Multiple fields on same plot\n        {\n          \"field\": \"stats.e2e_latency.mean\",\n          \"label\": \"Mean Latency\",              // Optional custom label\n          \"color\": \"blue\",                      // Optional custom color\n          \"linestyle\": \"-\"                      // Optional: '-', '--', '-.', ':'\n        },\n        {\n          \"field\": \"stats.e2e_latency.p90\",\n          \"label\": \"P90 Latency\",\n          \"color\": \"red\",\n          \"linestyle\": \"--\"\n        }\n      ],\n      \"x_label\": \"RPS\",\n      \"y_label\": \"Latency (s)\",\n      \"plot_type\": \"line\",\n      \"position\": [0, 0]\n    }\n  ]\n}\n</code></pre>"},{"location":"examples/plot-config-examples/#key-features","title":"Key Features","text":"<ul> <li>Single vs Multi-Line: Use <code>y_field</code> for single line, <code>y_fields</code> for multiple lines</li> <li>Custom Styling: Each line can have custom color, linestyle, and label</li> <li>Flexible Layout: Any NxM grid layout from 1x1 to 5x6</li> <li>Plot Types: <code>line</code>, <code>scatter</code>, <code>bar</code> (multi-line bar creates grouped bars)</li> <li>Automatic Legends: Multi-line plots automatically generate legends</li> </ul>"},{"location":"examples/plot-config-examples/#when-to-use-multi-line-plots","title":"When to Use Multi-Line Plots","text":"<p>\u2705 GOOD Use Cases: - Single scenario analysis: Use <code>--group-key \"\"</code> (empty string) for one traffic scenario - Deep metric comparison: Comparing mean, P90, P99 latency on same plot - Performance analysis: Related metrics on the same scale</p> <p>\u274c AVOID Multi-Line Plots When: - Multiple scenarios: <code>--group-key traffic_scenario</code> with multiple scenarios - Multiple server versions: <code>--group-key server_version</code> - Any grouping: Multi-line + grouping creates cluttered, hard-to-read plots</p> <p>The system will automatically convert multi-line plots to single-line plots when it detects multiple groups/scenarios for better visualization.</p>"},{"location":"examples/plot-config-examples/#usage-patterns","title":"Usage Patterns","text":"<pre><code># \u2705 GOOD: Multi-line for single scenario analysis\ngenai-bench plot --preset single_scenario_analysis --group-key \"\"\n\n# \u2705 GOOD: Single-line for multiple scenarios\ngenai-bench plot --preset 2x4_default --group-key traffic_scenario\n\n# \u26a0\ufe0f AUTO-CONVERTED: Multi-line + grouping \u2192 single-line\ngenai-bench plot --preset multi_line_latency --group-key traffic_scenario\n</code></pre>"},{"location":"examples/plot-config-examples/#available-fields","title":"Available Fields","text":"<p>Run <code>genai-bench plot --experiments-folder /path/to/experiments --group-key traffic_scenario --list-fields</code> to see all available field paths with actual data from your experiments.</p> <p>Common field paths include:</p>"},{"location":"examples/plot-config-examples/#direct-metrics","title":"Direct Metrics","text":"<ul> <li><code>num_concurrency</code> - Concurrency level</li> <li><code>requests_per_second</code> - RPS</li> <li><code>error_rate</code> - Error rate</li> <li><code>mean_output_throughput_tokens_per_s</code> - Server output throughput</li> <li><code>mean_total_tokens_throughput_tokens_per_s</code> - Total throughput</li> <li><code>run_duration</code> - Duration of the run</li> </ul>"},{"location":"examples/plot-config-examples/#statistical-fields","title":"Statistical Fields","text":"<p>Access statistics using <code>stats.{metric}.{statistic}</code>:</p> <p>Metrics: ttft, tpot, e2e_latency, output_latency, output_inference_speed, num_input_tokens, num_output_tokens, total_tokens, input_throughput, output_throughput</p> <p>Statistics: min, max, mean, stddev, sum, p25, p50, p75, p90, p95, p99</p> <p>Examples: - <code>stats.ttft.mean</code> - Mean time to first token - <code>stats.e2e_latency.p99</code> - 99<sup>th</sup> percentile end-to-end latency - <code>stats.output_inference_speed.mean</code> - Mean output inference speed</p>"},{"location":"examples/plot-config-examples/#built-in-presets","title":"Built-in Presets","text":""},{"location":"examples/plot-config-examples/#2x4_default","title":"2x4_default","text":"<p>The original 2x4 layout with all 8 standard plots. This maintains backwards compatibility with the existing plotting system.</p>"},{"location":"examples/plot-config-examples/#simple_2x2","title":"simple_2x2","text":"<p>A simplified 2x2 layout with the most important metrics for quick analysis.</p>"},{"location":"examples/plot-config-examples/#creating-custom-configurations","title":"Creating Custom Configurations","text":"<ol> <li>Start with an example configuration</li> <li>Modify the layout dimensions and plot specifications</li> <li>Use <code>--list-fields</code> to find available metrics</li> <li>Use <code>--validate-only</code> to test your configuration</li> <li>Generate plots with your custom config</li> </ol>"},{"location":"examples/plot-config-examples/#tips","title":"Tips","text":"<ul> <li>Use descriptive titles for your plots</li> <li>Choose appropriate plot types (line for trends, scatter for relationships, bar for comparisons)</li> <li>Ensure field paths are valid using <code>--validate-only</code></li> <li>Consider your audience when selecting metrics to display</li> <li>Use figsize to adjust the output image dimensions</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome to GenAI Bench! This section will help you get up and running quickly.</p>"},{"location":"getting-started/#overview","title":"Overview","text":"<p>GenAI Bench is a powerful benchmark tool designed for comprehensive token-level performance evaluation of large language model (LLM) serving systems. It supports multiple cloud providers for both model endpoints and storage.</p>"},{"location":"getting-started/#quick-links","title":"Quick Links","text":"<ul> <li> <p> Installation</p> <p>Get GenAI Bench installed on your system</p> <p> Installation Guide</p> </li> <li> <p> Task Types</p> <p>Learn about supported benchmark tasks</p> <p> Task Definition</p> </li> <li> <p> CLI Usage</p> <p>Master the command-line interface</p> <p> Command Guidelines</p> </li> <li> <p> Metrics</p> <p>Understand benchmark metrics</p> <p> Metrics Definition</p> </li> </ul>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ul> <li>Python 3.8 or higher</li> <li>Access to at least one supported LLM endpoint</li> <li>(Optional) Cloud storage credentials for result upload</li> </ul>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ol> <li>Install GenAI Bench</li> <li>Configure your authentication</li> <li>Run your first benchmark</li> </ol>"},{"location":"getting-started/command-guidelines/","title":"Command Guidelines","text":"<p>Once you install it in your local environment, you can use <code>--help</code> to read about what command options it supports.</p> <pre><code>genai-bench --help\n</code></pre> <p><code>genai-bench</code> supports three commands:</p> <pre><code>Commands:\n  benchmark  Run a benchmark based on user defined scenarios.\n  excel      Exports the experiment results to an Excel file.\n  plot       Plots the experiment(s) results based on filters and group...\n</code></pre> <p>You can also refer to option_groups.py.</p>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide covers all the ways to install GenAI Bench, from simple PyPI installation to full development setup.</p>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#method-1-pypi-installation-recommended","title":"Method 1: PyPI Installation (Recommended)","text":"<p>The simplest way to install GenAI Bench:</p> <pre><code>pip install genai-bench\n</code></pre> <p>For a specific version:</p> <pre><code>pip install genai-bench==0.1.75\n</code></pre>"},{"location":"getting-started/installation/#method-2-development-installation","title":"Method 2: Development Installation","text":"<p>For development or to use the latest features:</p> <ol> <li>Please make sure you have Python3.11 installed. You can check out online how to set it up.</li> <li>Use the virtual environment from uv</li> </ol> <p>Activate the virtual environment to ensure the dev environment is correctly set up:</p> <pre><code>make uv\nsource .venv/bin/activate\n</code></pre> <ol> <li>Install the Project in Editable Mode</li> </ol> <p>If not already done, install your project in editable mode using make. This ensures that any changes you make are immediately reflected:</p> <pre><code>make install\n</code></pre>"},{"location":"getting-started/installation/#method-3-docker-installation","title":"Method 3: Docker Installation","text":"<p>For containerized environments:</p> <p>Pull the latest docker image:</p> <pre><code>docker pull ghcr.io/moirai-internal/genai-bench:v0.0.1\n</code></pre>"},{"location":"getting-started/installation/#building-from-source","title":"Building from Source","text":"<p>Alternatively, you can build the image locally from the Dockerfile:</p> <pre><code>docker build . -f Dockerfile -t genai-bench:dev\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>After installation, verify that GenAI Bench is working:</p> <pre><code># Check version\ngenai-bench --version\n\n# Check help\ngenai-bench --help\n\n# Check benchmark command\ngenai-bench benchmark --help\n</code></pre>"},{"location":"getting-started/installation/#environment-setup","title":"Environment Setup","text":""},{"location":"getting-started/installation/#environment-variables","title":"Environment Variables","text":"<p>Set these environment variables for optimal performance:</p> <pre><code># For Hugging Face tokenizer downloads\nexport HF_TOKEN=\"your-huggingface-token\"\n\n# Disable torch warnings (not needed for benchmarking)\nexport TRANSFORMERS_VERBOSITY=error\n\n# Optional: Set log level\nexport GENAI_BENCH_LOG_LEVEL=INFO\n</code></pre>"},{"location":"getting-started/installation/#api-keys","title":"API Keys","text":"<p>Depending on your backend, you may need API keys:</p> <pre><code># OpenAI-compatible APIs\nexport OPENAI_API_KEY=\"your-api-key\"\n\n# Cohere API\nexport COHERE_API_KEY=\"your-cohere-key\"\n\n# OCI Cohere\nexport OCI_CONFIG_FILE=\"~/.oci/config\"\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/installation/#python-version-issues","title":"Python Version Issues","text":"<pre><code># Check Python version\npython3 --version\n\n# If you have multiple Python versions, use specific version\npython3.11 -m pip install genai-bench\n</code></pre>"},{"location":"getting-started/installation/#permission-issues","title":"Permission Issues","text":"<pre><code># Use user installation\npip install --user genai-bench\n\n# Or use virtual environment\npython3 -m venv genai-bench-env\nsource genai-bench-env/bin/activate\npip install genai-bench\n</code></pre>"},{"location":"getting-started/installation/#missing-dependencies","title":"Missing Dependencies","text":"<pre><code># Update pip\npip install --upgrade pip\n\n# Install with all dependencies\npip install genai-bench[dev]\n</code></pre>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the GitHub Issues</li> <li>Search for similar problems</li> <li>Create a new issue with:</li> <li>Your operating system and Python version</li> <li>Installation method used</li> <li>Full error message</li> <li>Steps to reproduce</li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>After successful installation:</p> <ol> <li>Read the Task Definition Guide to understand different benchmark tasks</li> <li>Explore the User Guide for detailed usage</li> <li>Check out Command Guidelines for practical scenarios </li> </ol>"},{"location":"getting-started/metrics-definition/","title":"Metrics Definition","text":"<p>This section puts together the standard metrics required for LLM serving performance analysis. We classify metrics to two types: single-request level metrics, representing the metrics collected from one request. And aggregated level metrics, summarizing the single-request metrics from one run (with specific traffic scenario and num concurrency).</p> <p>NOTE:</p> <ul> <li>Each single-request metric includes standard statistics: percentile, min, max, stddev, and mean.</li> <li>The following metrics cover input, output, and end-to-end (e2e) stages. For chat tasks, all stages are relevant for evaluation. For embedding tasks, where there is no output stage, output metrics will be set to 0. For details about output metrics collection, please check out <code>OUTPUT_METRICS_FIELDS</code> in metrics.py.</li> </ul>"},{"location":"getting-started/metrics-definition/#single-request-level-metrics","title":"Single Request Level Metrics","text":"<p>The following metrics capture token-level performance for a single request, providing insights into server efficiency for each individual request.</p> Glossary Meaning Calculation Formula Units TTFT Time to First Token. Initial response time when the first output token is generated.  This is also known as the latency for the input (input) stage. <code>ttft = time_at_first_token - start_time</code> seconds End-to-End Latency End-to-End latency. This metric indicates how long it takes from submitting a query to receiving the full response, including network latencies. <code>e2e_latency = end_time - start_time</code> seconds TPOT Time Per Output Token. The average time between two subsequent generated tokens. <code>TPOT = (e2e_latency - TTFT) / (num_output_tokens - 1)</code> seconds Output Inference Speed The rate of how many tokens the model can generate per second for a single request. <code>inference_speed = 1 / TPOT</code> tokens/second Num of Input Tokens Number of prompt tokens. <code>num_input_tokens = tokenizer.encode(prompt)</code> tokens Num of Output Tokens Number of output tokens. <code>num_output_tokens = num_completion_tokens</code> tokens Num of Request Tokens Total number of tokens processed in one request. <code>num_request_tokens = num_input_tokens + num_output_tokens</code> tokens Input Throughput The overall throughput of input (input process). <code>input_throughput = num_input_tokens / TTFT</code> tokens/second Output Throughput The throughput of output (output generation) for a single request. <code>output_throughput = (num_output_tokens - 1) / output_latency</code> tokens/second"},{"location":"getting-started/metrics-definition/#aggregated-metrics","title":"Aggregated Metrics","text":"<p>This metrics collection summarizes the metrics relevant to a specific traffic load pattern, defined by the traffic scenario and the num of concurrency. It provides insights into server capacity and performance under pressure.</p> Glossary Meaning Calculation Formula Units Mean Input Throughput The average throughput of how many input tokens can be processed by the model in one run with multiple concurrent requests. <code>mean_input_throughput = sum(input_tokens_for_all_requests) / run_duration</code> tokens/second Mean Output Throughput The average throughput of how many output tokens can be processed by the model in one run with multiple concurrent requests. <code>mean_output_throughput = sum(output_tokens_for_all_requests) / run_duration</code> tokens/second Total Tokens Throughput The average throughput of how many tokens can be processed by the model, including both input and output tokens. <code>mean_total_tokens_throughput = all_requests[\"total_tokens\"][\"sum\"] / run_duration</code> tokens/second Total Chars Per Hour<sup>1</sup> The average total characters can be processed by the model per hour. <code>total_chars_per_hour = total_tokens_throughput * dataset_chars_to_token_ratio * 3600</code> Characters Requests Per Minute The number of requests processed by the model per minute. <code>num_completed_requests_per_min = num_completed_requests / (end_time - start_time) * 60</code> Requests Error Codes to Frequency A map that shows the returned error status code to its frequency. Error Rate The rate of error requests over total requests. <code>error_rate = num_error_requests / num_requests</code> Num of Error Requests The number of error requests in one load. <pre><code>if requests.status_code != '200':  num_error_requests += 1</code></pre> Num of Completed Requests The number of completed requests in one load. <pre><code>if requests.status_code == '200':  num_completed_requests += 1</code></pre> Num of Requests The total number of requests processed for one load. <code>total_requests = num_completed_requests + num_error_requests</code> <ol> <li> <p>Total Chars Per Hour is derived from a character-to-token ratio based on sonnet.txt and the model's tokenizer. This metric aids in pricing decisions for an LLM serving solution. For tasks with multi-modal inputs, non-text tokens are converted to an equivalent character count using the same character-to-token ratio.\u00a0\u21a9</p> </li> </ol>"},{"location":"getting-started/task-definition/","title":"Task Definition","text":"<p>Tasks in <code>genai-bench</code> define the type of benchmark you want to run, based on the input modality (e.g., text, image) and output modality (e.g., text, embeddings). Tasks are specified using the <code>--task</code> option in the <code>genai-bench benchmark</code> command.</p> <p>Each task follows the pattern:</p> <pre><code>&lt;input_modality&gt;-to-&lt;output_modality&gt;\n</code></pre> <p>Here are the currently supported tasks:</p> <p>NOTE: Task compatibility may vary depending on the API format.</p> Task Name Description <code>text-to-text</code> Benchmarks generating text output from text input, such as chat or QA tasks. <code>text-to-embeddings</code> Benchmarks generating embeddings from text input, often for semantic search. <code>image-text-to-text</code> Benchmarks generating text from images and text prompts, such as visual question answering. <code>image-to-embeddings</code> Benchmarks generating embeddings from images, often for image similarity."},{"location":"getting-started/task-definition/#how-tasks-work","title":"How Tasks Work","text":"<ul> <li> <p>Input Modality: Defines the type of input data the task operates on, such as text or images.</p> </li> <li> <p>Output Modality: Defines the type of output the task generates, such as text or embeddings.</p> </li> </ul> <p>When you specify a task, the appropriate sampler (<code>TextSampler</code> or <code>ImageSampler</code>) and request type (<code>UserChatRequest</code>, <code>UserEmbeddingRequest</code>, etc.) are automatically selected based on the input and output modalities.</p>"},{"location":"getting-started/task-definition/#example-task-usage","title":"Example Task Usage","text":"<ul> <li> <p>For a text-to-text task (e.g., generating a response to a text prompt, typical chat completions):</p> <pre><code>genai-bench benchmark --task text-to-text ...\n</code></pre> </li> <li> <p>For an image-to-text task (e.g., generating a response for an image and text interleave message):</p> <pre><code>genai-bench benchmark --task image-to-text ...\n</code></pre> </li> <li> <p>For an image-to-embeddings task (e.g., generating embeddings for similarity search):</p> <pre><code>genai-bench benchmark --task text-to-embeddings ...\n</code></pre> </li> </ul>"},{"location":"user-guide/","title":"User Guide","text":"<p>This guide covers everything you need to know to effectively use GenAI Bench for benchmarking LLM endpoints.</p>"},{"location":"user-guide/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li> <p> Running Benchmarks</p> <p>Learn how to run benchmarks against various LLM endpoints</p> <p> Run Benchmark</p> </li> <li> <p> Multi-Cloud Setup</p> <p>Configure authentication for AWS, Azure, GCP, OCI, and more</p> <p> Multi-Cloud Guide</p> </li> <li> <p> Docker Deployment</p> <p>Run GenAI Bench in containerized environments</p> <p> Docker Guide</p> </li> <li> <p> Excel Reports</p> <p>Generate comprehensive Excel reports from benchmark results</p> <p> Excel Reports</p> </li> </ul>"},{"location":"user-guide/#common-workflows","title":"Common Workflows","text":""},{"location":"user-guide/#basic-benchmarking","title":"Basic Benchmarking","text":"<ol> <li>Choose your model provider - OpenAI, AWS Bedrock, Azure OpenAI, etc.</li> <li>Configure authentication - API keys, IAM roles, or service accounts</li> <li>Run the benchmark - Specify task type and parameters</li> <li>Analyze results - View real-time dashboard or generate reports</li> </ol>"},{"location":"user-guide/#cross-cloud-benchmarking","title":"Cross-Cloud Benchmarking","text":"<p>Benchmark models from one provider while storing results in another:</p> <pre><code># Benchmark OpenAI, store in AWS S3\ngenai-bench benchmark \\\n  --api-backend openai \\\n  --api-key $OPENAI_KEY \\\n  --upload-results \\\n  --storage-provider aws \\\n  --storage-bucket my-results\n</code></pre>"},{"location":"user-guide/#multi-modal-tasks","title":"Multi-Modal Tasks","text":"<p>Support for text, embeddings, and vision tasks:</p> <ul> <li><code>text-to-text</code> - Chat and completion tasks</li> <li><code>text-to-embeddings</code> - Embedding generation</li> <li><code>image-text-to-text</code> - Vision-language tasks</li> <li><code>text-to-rerank</code> - Document reranking</li> </ul>"},{"location":"user-guide/#need-help","title":"Need Help?","text":"<ul> <li>Check the Quick Reference for common commands</li> <li>Review Command Guidelines for detailed options</li> <li>See Troubleshooting for common issues</li> </ul>"},{"location":"user-guide/generate-excel-sheet/","title":"Generate an Excel sheet","text":"<p>genai-bench also provides the feature to analyze a finished benchmark. You can check out <code>genai-bench excel --help</code> to find how you can generate an <code>.xlsx</code> sheet containing a summary of your benchmark experiments.</p>"},{"location":"user-guide/generate-excel-sheet/#sample-command","title":"Sample command","text":"<pre><code>genai-bench excel --experiment-folder &lt;path-to-experiment-folder&gt; --excel-name &lt;name-of-the-sheet&gt;\n</code></pre>"},{"location":"user-guide/generate-plot/","title":"Generate a 2x4 Plot","text":"<p>You can check out <code>genai-bench plot --help</code> to find how to generate a 2x4 Plot containing:</p> <ol> <li>Output Inference Speed (tokens/s) vs Output Throughput of Server (tokens/s)</li> <li>TTFT (s) vs Output Throughput of Server (tokens/s)</li> <li>Mean E2E Latency (s) per Request vs RPS</li> <li>Error Rates by HTTP Status vs Concurrency</li> <li>Output Inference Speed per Request (tokens/s) vs Total Throughput (Input + Output) of Server (tokens/s)</li> <li>TTFT (s) vs Total Throughput (Input + Output) of Server (tokens/s)</li> <li>P90 E2E Latency (s) per Request vs RPS</li> <li>P99 E2E Latency (s) per Request vs RPS</li> </ol> <p>Note: TTFT plots automatically use logarithmic scale for better visualization of the wide range of values. You can override this by specifying <code>\"y_scale\": \"linear\"</code> in custom plot configurations.</p> <pre><code>genai-bench plot --experiments-folder &lt;path-to-experiment-folder&gt; --group-key traffic_scenario\n</code></pre>"},{"location":"user-guide/multi-cloud-auth-storage/","title":"Multi-Cloud Authentication and Storage Guide","text":"<p>genai-bench now supports comprehensive multi-cloud authentication for both model endpoints and storage services. This guide covers how to configure and use authentication for various cloud providers.</p>"},{"location":"user-guide/multi-cloud-auth-storage/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Model Provider Authentication</li> <li>OpenAI</li> <li>OCI Cohere</li> <li>AWS Bedrock</li> <li>Azure OpenAI</li> <li>GCP Vertex AI</li> <li>SGLang / vLLM</li> <li>Storage Provider Authentication</li> <li>OCI Object Storage</li> <li>AWS S3</li> <li>Azure Blob Storage</li> <li>GCP Cloud Storage</li> <li>GitHub Releases</li> <li>Command Examples</li> <li>Environment Variables</li> <li>Best Practices</li> </ul>"},{"location":"user-guide/multi-cloud-auth-storage/#overview","title":"Overview","text":"<p>genai-bench separates authentication into two categories: 1. Model Authentication: For accessing LLM endpoints 2. Storage Authentication: For uploading benchmark results</p> <p>This separation allows you to benchmark models from one provider while storing results in another provider's storage service.</p>"},{"location":"user-guide/multi-cloud-auth-storage/#model-provider-authentication","title":"Model Provider Authentication","text":""},{"location":"user-guide/multi-cloud-auth-storage/#openai","title":"OpenAI","text":"<p>OpenAI uses API key authentication.</p> <p>Required parameters: - <code>--api-backend openai</code> - <code>--api-key</code> or <code>--model-api-key</code>: Your OpenAI API key</p> <p>Example: <pre><code>genai-bench benchmark \\\n  --api-backend openai \\\n  --api-base https://api.openai.com/v1 \\\n  --api-key sk-... \\\n  --api-model-name gpt-4 \\\n  --model-tokenizer gpt2 \\\n  --task text-to-text \\\n  --max-requests-per-run 100 \\\n  --max-time-per-run 10\n</code></pre></p> <p>Environment variable alternative: <pre><code>export MODEL_API_KEY=sk-...\ngenai-bench benchmark --api-backend openai ...\n</code></pre></p>"},{"location":"user-guide/multi-cloud-auth-storage/#oci-cohere","title":"OCI Cohere","text":"<p>OCI supports multiple authentication methods.</p> <p>Authentication types: - <code>user_principal</code>: Default, uses OCI config file - <code>instance_principal</code>: For compute instances - <code>security_token</code>: For delegation tokens - <code>instance_obo_user</code>: Instance principal with user delegation</p> <p>Required parameters: - <code>--api-backend oci-cohere</code> or <code>--api-backend cohere</code> - <code>--auth</code>: Authentication type (default: user_principal)</p> <p>User Principal example: <pre><code>genai-bench benchmark \\\n  --api-backend oci-cohere \\\n  --api-base https://inference.generativeai.us-chicago-1.oci.oraclecloud.com \\\n  --auth user_principal \\\n  --config-file ~/.oci/config \\\n  --profile DEFAULT \\\n  --api-model-name cohere.command-r-plus \\\n  --model-tokenizer Cohere/command-r-plus \\\n  --task text-to-text \\\n  --max-requests-per-run 100 \\\n  --max-time-per-run 10\n</code></pre></p> <p>Instance Principal example: <pre><code>genai-bench benchmark \\\n  --api-backend oci-cohere \\\n  --api-base https://inference.generativeai.us-chicago-1.oci.oraclecloud.com \\\n  --auth instance_principal \\\n  --region us-chicago-1 \\\n  --api-model-name cohere.command-r-plus \\\n  --model-tokenizer Cohere/command-r-plus \\\n  --task text-to-text \\\n  --max-requests-per-run 100 \\\n  --max-time-per-run 10\n</code></pre></p>"},{"location":"user-guide/multi-cloud-auth-storage/#aws-bedrock","title":"AWS Bedrock","text":"<p>AWS Bedrock supports IAM credentials and profiles.</p> <p>Authentication methods: 1. IAM Credentials: Access key ID and secret access key 2. AWS Profile: Named profile from credentials file 3. Environment variables: AWS SDK default behavior</p> <p>Required parameters: - <code>--api-backend aws-bedrock</code> - <code>--aws-region</code>: AWS region for Bedrock</p> <p>IAM Credentials example: <pre><code>genai-bench benchmark \\\n  --api-backend aws-bedrock \\\n  --api-base https://bedrock-runtime.us-east-1.amazonaws.com \\\n  --aws-access-key-id AKIAIOSFODNN7EXAMPLE \\\n  --aws-secret-access-key wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY \\\n  --aws-region us-east-1 \\\n  --api-model-name anthropic.claude-3-sonnet-20240229-v1:0 \\\n  --model-tokenizer Anthropic/claude-3-sonnet \\\n  --task text-to-text \\\n  --max-requests-per-run 100 \\\n  --max-time-per-run 10\n</code></pre></p> <p>AWS Profile example: <pre><code>genai-bench benchmark \\\n  --api-backend aws-bedrock \\\n  --api-base https://bedrock-runtime.us-west-2.amazonaws.com \\\n  --aws-profile production \\\n  --aws-region us-west-2 \\\n  --api-model-name amazon.titan-text-express-v1 \\\n  --model-tokenizer amazon/titan \\\n  --task text-to-text \\\n  --max-requests-per-run 100 \\\n  --max-time-per-run 10\n</code></pre></p> <p>Environment variables: <pre><code>export AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_DEFAULT_REGION=us-east-1\n\ngenai-bench benchmark --api-backend aws-bedrock ...\n</code></pre></p>"},{"location":"user-guide/multi-cloud-auth-storage/#azure-openai","title":"Azure OpenAI","text":"<p>Azure OpenAI supports API key and Azure AD authentication.</p> <p>Authentication methods: 1. API Key: Traditional API key authentication 2. Azure AD: Azure Active Directory token</p> <p>Required parameters: - <code>--api-backend azure-openai</code> - <code>--azure-endpoint</code>: Your Azure OpenAI endpoint - <code>--azure-deployment</code>: Your deployment name - <code>--azure-api-version</code>: API version (default: 2024-02-01)</p> <p>API Key example: <pre><code>genai-bench benchmark \\\n  --api-backend azure-openai \\\n  --api-base https://myresource.openai.azure.com \\\n  --azure-endpoint https://myresource.openai.azure.com \\\n  --azure-deployment my-gpt-4-deployment \\\n  --model-api-key YOUR_AZURE_API_KEY \\\n  --api-model-name gpt-4 \\\n  --model-tokenizer gpt2 \\\n  --task text-to-text \\\n  --max-requests-per-run 100 \\\n  --max-time-per-run 10\n</code></pre></p> <p>Azure AD example: <pre><code>genai-bench benchmark \\\n  --api-backend azure-openai \\\n  --api-base https://myresource.openai.azure.com \\\n  --azure-endpoint https://myresource.openai.azure.com \\\n  --azure-deployment my-gpt-4-deployment \\\n  --azure-ad-token YOUR_AAD_TOKEN \\\n  --api-model-name gpt-4 \\\n  --model-tokenizer gpt2 \\\n  --task text-to-text \\\n  --max-requests-per-run 100 \\\n  --max-time-per-run 10\n</code></pre></p>"},{"location":"user-guide/multi-cloud-auth-storage/#gcp-vertex-ai","title":"GCP Vertex AI","text":"<p>GCP Vertex AI supports service account and API key authentication.</p> <p>Authentication methods: 1. Service Account: JSON key file 2. API Key: For certain Vertex AI services 3. Application Default Credentials: GCP SDK default</p> <p>Required parameters: - <code>--api-backend gcp-vertex</code> - <code>--gcp-project-id</code>: Your GCP project ID - <code>--gcp-location</code>: GCP region (default: us-central1)</p> <p>Service Account example: <pre><code>genai-bench benchmark \\\n  --api-backend gcp-vertex \\\n  --api-base https://us-central1-aiplatform.googleapis.com \\\n  --gcp-project-id my-project-123 \\\n  --gcp-location us-central1 \\\n  --gcp-credentials-path /path/to/service-account.json \\\n  --api-model-name gemini-1.5-pro \\\n  --model-tokenizer google/gemini \\\n  --task text-to-text \\\n  --max-requests-per-run 100 \\\n  --max-time-per-run 10\n</code></pre></p> <p>Environment variable: <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json\nexport GCP_PROJECT_ID=my-project-123\n\ngenai-bench benchmark --api-backend gcp-vertex ...\n</code></pre></p>"},{"location":"user-guide/multi-cloud-auth-storage/#sglang-or-vllm","title":"SGLang or vLLM","text":"<p>vLLM and SGLang use OpenAI-compatible APIs with optional authentication.</p> <p>Required parameters: - <code>--api-backend sglang</code> or <code>--api-backend vllm</code> - <code>--api-base</code>: Your server endpoint - <code>--api-key</code> or <code>--model-api-key</code>: Optional API key if authentication is enabled</p> <p>Example: <pre><code>genai-bench benchmark \\\n  --api-backend vllm \\\n  --api-base http://localhost:8000 \\\n  --api-key optional-key \\\n  --api-model-name meta-llama/Llama-2-7b-hf \\\n  --model-tokenizer meta-llama/Llama-2-7b-hf \\\n  --task text-to-text \\\n  --max-requests-per-run 100 \\\n  --max-time-per-run 10\n</code></pre></p>"},{"location":"user-guide/multi-cloud-auth-storage/#storage-provider-authentication","title":"Storage Provider Authentication","text":"<p>Storage authentication is configured separately from model authentication, allowing you to store results in any supported storage service.</p>"},{"location":"user-guide/multi-cloud-auth-storage/#common-storage-parameters","title":"Common Storage Parameters","text":"<p>All storage providers share these common parameters: - <code>--upload-results</code>: Flag to enable result upload - <code>--storage-provider</code>: Storage provider type (oci, aws, azure, gcp, github) - <code>--storage-bucket</code>: Bucket/container name - <code>--storage-prefix</code>: Optional prefix for uploaded objects</p>"},{"location":"user-guide/multi-cloud-auth-storage/#oci-object-storage","title":"OCI Object Storage","text":"<p>Authentication types: Same as OCI model authentication (user_principal, instance_principal, etc.)</p> <p>Example: <pre><code>genai-bench benchmark \\\n  ... \\\n  --upload-results \\\n  --storage-provider oci \\\n  --storage-bucket my-benchmark-results \\\n  --storage-prefix experiments/2024 \\\n  --storage-auth-type user_principal \\\n  --namespace my-namespace\n</code></pre></p>"},{"location":"user-guide/multi-cloud-auth-storage/#aws-s3","title":"AWS S3","text":"<p>Authentication methods: 1. IAM Credentials 2. AWS Profile 3. Environment variables</p> <p>IAM Credentials example: <pre><code>genai-bench benchmark \\\n  ... \\\n  --upload-results \\\n  --storage-provider aws \\\n  --storage-bucket my-benchmark-results \\\n  --storage-prefix experiments/2024 \\\n  --storage-aws-access-key-id AKIAIOSFODNN7EXAMPLE \\\n  --storage-aws-secret-access-key wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY \\\n  --storage-aws-region us-east-1\n</code></pre></p> <p>AWS Profile example: <pre><code>genai-bench benchmark \\\n  ... \\\n  --upload-results \\\n  --storage-provider aws \\\n  --storage-bucket my-benchmark-results \\\n  --storage-prefix experiments/2024 \\\n  --storage-aws-profile production \\\n  --storage-aws-region us-west-2\n</code></pre></p>"},{"location":"user-guide/multi-cloud-auth-storage/#azure-blob-storage","title":"Azure Blob Storage","text":"<p>Authentication methods: 1. Storage Account Key 2. Connection String 3. SAS Token 4. Azure AD</p> <p>Account Key example: <pre><code>genai-bench benchmark \\\n  ... \\\n  --upload-results \\\n  --storage-provider azure \\\n  --storage-bucket my-container \\\n  --storage-prefix experiments/2024 \\\n  --storage-azure-account-name mystorageaccount \\\n  --storage-azure-account-key YOUR_ACCOUNT_KEY\n</code></pre></p> <p>Connection String example: <pre><code>genai-bench benchmark \\\n  ... \\\n  --upload-results \\\n  --storage-provider azure \\\n  --storage-bucket my-container \\\n  --storage-azure-connection-string \"DefaultEndpointsProtocol=https;AccountName=...\"\n</code></pre></p>"},{"location":"user-guide/multi-cloud-auth-storage/#gcp-cloud-storage","title":"GCP Cloud Storage","text":"<p>Authentication methods: 1. Service Account 2. Application Default Credentials</p> <p>Example: <pre><code>genai-bench benchmark \\\n  ... \\\n  --upload-results \\\n  --storage-provider gcp \\\n  --storage-bucket my-benchmark-results \\\n  --storage-prefix experiments/2024 \\\n  --storage-gcp-project-id my-project-123 \\\n  --storage-gcp-credentials-path /path/to/service-account.json\n</code></pre></p>"},{"location":"user-guide/multi-cloud-auth-storage/#github-releases","title":"GitHub Releases","text":"<p>GitHub storage uploads results as release artifacts.</p> <p>Required parameters: - <code>--github-token</code>: Personal access token with repo permissions - <code>--github-owner</code>: Repository owner (user or organization) - <code>--github-repo</code>: Repository name</p> <p>Example: <pre><code>genai-bench benchmark \\\n  ... \\\n  --upload-results \\\n  --storage-provider github \\\n  --github-token ghp_xxxxxxxxxxxxxxxxxxxx \\\n  --github-owner myorg \\\n  --github-repo benchmark-results\n</code></pre></p>"},{"location":"user-guide/multi-cloud-auth-storage/#command-examples","title":"Command Examples","text":""},{"location":"user-guide/multi-cloud-auth-storage/#cross-cloud-benchmarking","title":"Cross-Cloud Benchmarking","text":"<p>Benchmark OpenAI and store in AWS S3: <pre><code>genai-bench benchmark \\\n  --api-backend openai \\\n  --api-base https://api.openai.com/v1 \\\n  --api-key sk-... \\\n  --api-model-name gpt-4 \\\n  --model-tokenizer gpt2 \\\n  --task text-to-text \\\n  --max-requests-per-run 100 \\\n  --max-time-per-run 10 \\\n  --upload-results \\\n  --storage-provider aws \\\n  --storage-bucket my-benchmarks \\\n  --storage-aws-profile default \\\n  --storage-aws-region us-east-1\n</code></pre></p> <p>Benchmark AWS Bedrock and store in Azure Blob: <pre><code>genai-bench benchmark \\\n  --api-backend aws-bedrock \\\n  --api-base https://bedrock-runtime.us-east-1.amazonaws.com \\\n  --aws-profile bedrock-user \\\n  --aws-region us-east-1 \\\n  --api-model-name anthropic.claude-3-sonnet-20240229-v1:0 \\\n  --model-tokenizer Anthropic/claude-3-sonnet \\\n  --task text-to-text \\\n  --max-requests-per-run 100 \\\n  --max-time-per-run 10 \\\n  --upload-results \\\n  --storage-provider azure \\\n  --storage-bucket benchmarks \\\n  --storage-azure-connection-string \"DefaultEndpointsProtocol=...\"\n</code></pre></p>"},{"location":"user-guide/multi-cloud-auth-storage/#multi-modal-tasks","title":"Multi-Modal Tasks","text":"<p>Image-to-text with GCP Vertex AI: <pre><code>genai-bench benchmark \\\n  --api-backend gcp-vertex \\\n  --api-base https://us-central1-aiplatform.googleapis.com \\\n  --gcp-project-id my-project \\\n  --gcp-location us-central1 \\\n  --gcp-credentials-path /path/to/service-account.json \\\n  --api-model-name gemini-1.5-pro-vision \\\n  --model-tokenizer google/gemini \\\n  --task image-text-to-text \\\n  --dataset-path /path/to/image/dataset \\\n  --max-requests-per-run 50 \\\n  --max-time-per-run 10\n</code></pre></p>"},{"location":"user-guide/multi-cloud-auth-storage/#environment-variables","title":"Environment Variables","text":"<p>genai-bench supports environment variables for sensitive credentials:</p>"},{"location":"user-guide/multi-cloud-auth-storage/#model-authentication","title":"Model Authentication","text":"<ul> <li><code>MODEL_API_KEY</code>: API key for OpenAI, Azure OpenAI, or GCP</li> <li><code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, <code>AWS_SESSION_TOKEN</code>: AWS credentials</li> <li><code>AWS_PROFILE</code>, <code>AWS_DEFAULT_REGION</code>: AWS configuration</li> <li><code>AZURE_OPENAI_ENDPOINT</code>, <code>AZURE_OPENAI_DEPLOYMENT</code>, <code>AZURE_OPENAI_API_VERSION</code>: Azure configuration</li> <li><code>AZURE_AD_TOKEN</code>: Azure AD authentication token</li> <li><code>GCP_PROJECT_ID</code>, <code>GCP_LOCATION</code>: GCP configuration</li> <li><code>GOOGLE_APPLICATION_CREDENTIALS</code>: Path to GCP service account JSON</li> </ul>"},{"location":"user-guide/multi-cloud-auth-storage/#storage-authentication","title":"Storage Authentication","text":"<ul> <li><code>AZURE_STORAGE_ACCOUNT_NAME</code>, <code>AZURE_STORAGE_ACCOUNT_KEY</code>: Azure storage credentials</li> <li><code>AZURE_STORAGE_CONNECTION_STRING</code>, <code>AZURE_STORAGE_SAS_TOKEN</code>: Azure storage alternatives</li> <li><code>GITHUB_TOKEN</code>, <code>GITHUB_OWNER</code>, <code>GITHUB_REPO</code>: GitHub configuration</li> </ul>"},{"location":"user-guide/multi-cloud-auth-storage/#general","title":"General","text":"<ul> <li><code>HF_TOKEN</code>: HuggingFace token for downloading tokenizers</li> </ul>"},{"location":"user-guide/multi-cloud-auth-storage/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/multi-cloud-auth-storage/#security","title":"Security","text":"<ol> <li>Never commit credentials: Use environment variables or secure credential stores</li> <li>Use least privilege: Grant only necessary permissions for benchmarking</li> <li>Rotate credentials regularly: Update API keys and tokens periodically</li> <li>Use service accounts: Prefer service accounts over personal credentials for automation</li> </ol>"},{"location":"user-guide/multi-cloud-auth-storage/#performance","title":"Performance","text":"<ol> <li>Choose nearby regions: Select cloud regions close to your location for lower latency</li> <li>Batch operations: Use appropriate batch sizes for embedding tasks</li> <li>Monitor costs: Be aware of API pricing and set appropriate limits</li> </ol>"},{"location":"user-guide/multi-cloud-auth-storage/#organization","title":"Organization","text":"<ol> <li>Use consistent naming: Adopt a naming convention for storage prefixes</li> <li>Separate environments: Use different buckets/prefixes for dev/test/prod</li> <li>Tag resources: Use cloud provider tags for cost tracking and organization</li> </ol>"},{"location":"user-guide/multi-cloud-auth-storage/#important-notes","title":"Important Notes","text":"<ol> <li>Task-specific behavior:</li> <li>For <code>text-to-embeddings</code> and <code>text-to-rerank</code> tasks, the iteration type automatically switches to <code>batch_size</code></li> <li>For other tasks, <code>num_concurrency</code> iteration is used</li> <li> <p>This is handled automatically by the CLI</p> </li> <li> <p>Image format requirements:</p> </li> <li>Image inputs are expected to be in JPEG format for multi-modal tasks</li> <li> <p>Base64 encoding is handled automatically</p> </li> <li> <p>Token counting:</p> </li> <li>Different providers may use different tokenization methods</li> <li>Token estimates for embeddings tasks may vary by provider</li> </ol>"},{"location":"user-guide/multi-cloud-auth-storage/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>Check credentials: Verify authentication credentials are correct</li> <li>Verify permissions: Ensure accounts have necessary permissions</li> <li>Check regions: Confirm services are available in selected regions</li> <li>Review quotas: Check API quotas and rate limits</li> <li>Enable logging: Use verbose logging for debugging authentication issues</li> </ol>"},{"location":"user-guide/multi-cloud-auth-storage/#migration-from-legacy-cli","title":"Migration from Legacy CLI","text":"<p>If you're migrating from the legacy OCI-only CLI:</p> <p>Old command: <pre><code>genai-bench benchmark \\\n  --api-backend oci-cohere \\\n  --bucket my-bucket \\\n  --prefix my-prefix \\\n  ...\n</code></pre></p> <p>New command: <pre><code>genai-bench benchmark \\\n  --api-backend oci-cohere \\\n  --storage-bucket my-bucket \\\n  --storage-prefix my-prefix \\\n  --storage-provider oci \\\n  ...\n</code></pre></p> <p>The main changes are: - <code>--bucket</code> \u2192 <code>--storage-bucket</code> - <code>--prefix</code> \u2192 <code>--storage-prefix</code> - Add <code>--storage-provider oci</code> (though OCI is the default for backward compatibility)</p>"},{"location":"user-guide/multi-cloud-quick-reference/","title":"Multi-Cloud Quick Reference","text":"<p>This is a quick reference guide for common multi-cloud scenarios with genai-bench. For detailed information, see the comprehensive guide.</p> <p>Note: For OpenAI, vLLM, and SGLang backends, both <code>--api-key</code> and <code>--model-api-key</code> are supported for backward compatibility.</p>"},{"location":"user-guide/multi-cloud-quick-reference/#openai-benchmarking","title":"OpenAI Benchmarking","text":""},{"location":"user-guide/multi-cloud-quick-reference/#basic-usage","title":"Basic Usage","text":"<pre><code>genai-bench benchmark \\\n  --api-backend openai \\\n  --api-base https://api.openai.com/v1 \\\n  --api-key sk-... \\\n  --api-model-name gpt-4 \\\n  --model-tokenizer gpt2 \\\n  --task text-to-text \\\n  --max-requests-per-run 100 \\\n  --max-time-per-run 10\n</code></pre>"},{"location":"user-guide/multi-cloud-quick-reference/#with-environment-variable","title":"With Environment Variable","text":"<pre><code>export MODEL_API_KEY=sk-...\ngenai-bench benchmark \\\n  --api-backend openai \\\n  --api-base https://api.openai.com/v1 \\\n  --api-model-name gpt-4 \\\n  --model-tokenizer gpt2 \\\n  --task text-to-text \\\n  --max-requests-per-run 100 \\\n  --max-time-per-run 10\n</code></pre>"},{"location":"user-guide/multi-cloud-quick-reference/#aws-bedrock-benchmarking","title":"AWS Bedrock Benchmarking","text":""},{"location":"user-guide/multi-cloud-quick-reference/#using-aws-profile","title":"Using AWS Profile","text":"<pre><code>genai-bench benchmark \\\n  --api-backend aws-bedrock \\\n  --api-base https://bedrock-runtime.us-east-1.amazonaws.com \\\n  --aws-profile default \\\n  --aws-region us-east-1 \\\n  --api-model-name anthropic.claude-3-sonnet-20240229-v1:0 \\\n  --model-tokenizer Anthropic/claude-3-sonnet \\\n  --task text-to-text \\\n  --max-requests-per-run 100 \\\n  --max-time-per-run 10\n</code></pre>"},{"location":"user-guide/multi-cloud-quick-reference/#using-iam-credentials","title":"Using IAM Credentials","text":"<pre><code>genai-bench benchmark \\\n  --api-backend aws-bedrock \\\n  --api-base https://bedrock-runtime.us-east-1.amazonaws.com \\\n  --aws-access-key-id AKIAIOSFODNN7EXAMPLE \\\n  --aws-secret-access-key wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY \\\n  --aws-region us-east-1 \\\n  --api-model-name amazon.titan-text-express-v1 \\\n  --model-tokenizer amazon/titan \\\n  --task text-to-text \\\n  --max-requests-per-run 100 \\\n  --max-time-per-run 10\n</code></pre>"},{"location":"user-guide/multi-cloud-quick-reference/#azure-openai-benchmarking","title":"Azure OpenAI Benchmarking","text":""},{"location":"user-guide/multi-cloud-quick-reference/#using-api-key","title":"Using API Key","text":"<pre><code>genai-bench benchmark \\\n  --api-backend azure-openai \\\n  --api-base https://myresource.openai.azure.com \\\n  --azure-endpoint https://myresource.openai.azure.com \\\n  --azure-deployment my-gpt-4-deployment \\\n  --model-api-key YOUR_API_KEY \\\n  --api-model-name gpt-4 \\\n  --model-tokenizer gpt2 \\\n  --task text-to-text \\\n  --max-requests-per-run 100 \\\n  --max-time-per-run 10\n</code></pre>"},{"location":"user-guide/multi-cloud-quick-reference/#gcp-vertex-ai-benchmarking","title":"GCP Vertex AI Benchmarking","text":""},{"location":"user-guide/multi-cloud-quick-reference/#using-service-account","title":"Using Service Account","text":"<pre><code>export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json\ngenai-bench benchmark \\\n  --api-backend gcp-vertex \\\n  --api-base https://us-central1-aiplatform.googleapis.com \\\n  --gcp-project-id my-project-123 \\\n  --gcp-location us-central1 \\\n  --api-model-name gemini-1.5-pro \\\n  --model-tokenizer google/gemini \\\n  --task text-to-text \\\n  --max-requests-per-run 100 \\\n  --max-time-per-run 10\n</code></pre>"},{"location":"user-guide/multi-cloud-quick-reference/#storage-examples","title":"Storage Examples","text":""},{"location":"user-guide/multi-cloud-quick-reference/#upload-to-oci-object-storage","title":"Upload to OCI Object Storage","text":"<pre><code>genai-bench benchmark \\\n  ... \\\n  --upload-results \\\n  --storage-provider oci \\\n  --storage-bucket my-benchmarks \\\n  --storage-prefix experiments/2024 \\\n  --namespace my-namespace\n</code></pre>"},{"location":"user-guide/multi-cloud-quick-reference/#upload-to-aws-s3","title":"Upload to AWS S3","text":"<pre><code>genai-bench benchmark \\\n  ... \\\n  --upload-results \\\n  --storage-provider aws \\\n  --storage-bucket my-benchmarks \\\n  --storage-prefix experiments/2024 \\\n  --storage-aws-profile default\n</code></pre>"},{"location":"user-guide/multi-cloud-quick-reference/#upload-to-azure-blob","title":"Upload to Azure Blob","text":"<pre><code>genai-bench benchmark \\\n  ... \\\n  --upload-results \\\n  --storage-provider azure \\\n  --storage-bucket my-container \\\n  --storage-azure-account-name myaccount \\\n  --storage-azure-account-key YOUR_ACCOUNT_KEY\n</code></pre>"},{"location":"user-guide/multi-cloud-quick-reference/#upload-to-gcp-cloud-storage","title":"Upload to GCP Cloud Storage","text":"<pre><code>genai-bench benchmark \\\n  ... \\\n  --upload-results \\\n  --storage-provider gcp \\\n  --storage-bucket my-benchmarks \\\n  --storage-gcp-project-id my-project \\\n  --storage-gcp-credentials-path /path/to/service-account.json\n</code></pre>"},{"location":"user-guide/multi-cloud-quick-reference/#cross-cloud-examples","title":"Cross-Cloud Examples","text":""},{"location":"user-guide/multi-cloud-quick-reference/#benchmark-openai-store-in-s3","title":"Benchmark OpenAI, Store in S3","text":"<pre><code>export MODEL_API_KEY=sk-...\nexport AWS_PROFILE=default\n\ngenai-bench benchmark \\\n  --api-backend openai \\\n  --api-base https://api.openai.com/v1 \\\n  --api-model-name gpt-4 \\\n  --model-tokenizer gpt2 \\\n  --task text-to-text \\\n  --max-requests-per-run 100 \\\n  --max-time-per-run 10 \\\n  --upload-results \\\n  --storage-provider aws \\\n  --storage-bucket openai-benchmarks \\\n  --storage-aws-region us-east-1\n</code></pre>"},{"location":"user-guide/multi-cloud-quick-reference/#benchmark-bedrock-store-in-azure","title":"Benchmark Bedrock, Store in Azure","text":"<pre><code>export AWS_PROFILE=bedrock-user\nexport AZURE_STORAGE_CONNECTION_STRING=\"DefaultEndpointsProtocol=...\"\n\ngenai-bench benchmark \\\n  --api-backend aws-bedrock \\\n  --api-base https://bedrock-runtime.us-east-1.amazonaws.com \\\n  --aws-region us-east-1 \\\n  --api-model-name anthropic.claude-3-sonnet-20240229-v1:0 \\\n  --model-tokenizer Anthropic/claude-3-sonnet \\\n  --task text-to-text \\\n  --max-requests-per-run 100 \\\n  --max-time-per-run 10 \\\n  --upload-results \\\n  --storage-provider azure \\\n  --storage-bucket bedrock-benchmarks\n</code></pre>"},{"location":"user-guide/multi-cloud-quick-reference/#benchmark-azure-openai-store-in-gcp","title":"Benchmark Azure OpenAI, Store in GCP","text":"<pre><code>export GOOGLE_APPLICATION_CREDENTIALS=/path/to/storage-sa.json\n\ngenai-bench benchmark \\\n  --api-backend azure-openai \\\n  --api-base https://myresource.openai.azure.com \\\n  --azure-endpoint https://myresource.openai.azure.com \\\n  --azure-deployment my-deployment \\\n  --model-api-key YOUR_API_KEY\n  --api-model-name gpt-4 \\\n  --model-tokenizer gpt2 \\\n  --task text-to-text \\\n  --max-requests-per-run 100 \\\n  --max-time-per-run 10 \\\n  --upload-results \\\n  --storage-provider gcp \\\n  --storage-bucket azure-benchmarks \\\n  --storage-gcp-project-id my-project\n</code></pre>"},{"location":"user-guide/multi-cloud-quick-reference/#multi-modal-tasks","title":"Multi-Modal Tasks","text":""},{"location":"user-guide/multi-cloud-quick-reference/#image-to-text-benchmarking","title":"Image-to-Text Benchmarking","text":"<pre><code>genai-bench benchmark \\\n  --api-backend gcp-vertex \\\n  --api-base https://us-central1-aiplatform.googleapis.com \\\n  --gcp-project-id my-project \\\n  --gcp-location us-central1 \\\n  --gcp-credentials-path /path/to/service-account.json \\\n  --api-model-name gemini-1.5-pro-vision \\\n  --model-tokenizer google/gemini \\\n  --task image-text-to-text \\\n  --dataset-path /path/to/images \\\n  --max-requests-per-run 50 \\\n  --max-time-per-run 10\n</code></pre>"},{"location":"user-guide/multi-cloud-quick-reference/#text-to-embeddings-benchmarking","title":"Text-to-Embeddings Benchmarking","text":"<pre><code>genai-bench benchmark \\\n  --api-backend openai \\\n  --api-base https://api.openai.com/v1 \\\n  --api-key sk-... \\\n  --api-model-name text-embedding-3-large \\\n  --model-tokenizer cl100k_base \\\n  --task text-to-embeddings \\\n  --batch-size 1 --batch-size 8 --batch-size 32 \\\n  --max-requests-per-run 1000 \\\n  --max-time-per-run 10\n</code></pre>"},{"location":"user-guide/multi-cloud-quick-reference/#environment-variable-reference","title":"Environment Variable Reference","text":""},{"location":"user-guide/multi-cloud-quick-reference/#model-authentication","title":"Model Authentication","text":"<pre><code># OpenAI\nexport MODEL_API_KEY=sk-...\n\n# AWS\nexport AWS_ACCESS_KEY_ID=AKIA...\nexport AWS_SECRET_ACCESS_KEY=...\nexport AWS_DEFAULT_REGION=us-east-1\nexport AWS_PROFILE=default\n\n# Azure\nexport AZURE_OPENAI_ENDPOINT=https://myresource.openai.azure.com\nexport AZURE_OPENAI_API_VERSION=2024-02-01\nexport AZURE_AD_TOKEN=...\n\n# GCP\nexport GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json\nexport GCP_PROJECT_ID=my-project\nexport GCP_LOCATION=us-central1\n</code></pre>"},{"location":"user-guide/multi-cloud-quick-reference/#storage-authentication","title":"Storage Authentication","text":"<pre><code># Azure Storage\nexport AZURE_STORAGE_ACCOUNT_NAME=myaccount\nexport AZURE_STORAGE_ACCOUNT_KEY=...\nexport AZURE_STORAGE_CONNECTION_STRING=...\n\n# GitHub\nexport GITHUB_TOKEN=ghp_...\nexport GITHUB_OWNER=myorg\nexport GITHUB_REPO=benchmarks\n</code></pre>"},{"location":"user-guide/multi-cloud-quick-reference/#general","title":"General","text":"<pre><code># HuggingFace (for downloading tokenizers)\nexport HF_TOKEN=hf_...\n</code></pre>"},{"location":"user-guide/run-benchmark-using-docker/","title":"Running Benchmark Using <code>genai-bench</code> Container","text":""},{"location":"user-guide/run-benchmark-using-docker/#using-pre-built-docker-image","title":"Using Pre-built Docker Image","text":"<p>Pull the latest docker image:</p> <pre><code>docker pull ghcr.io/moirai-internal/genai-bench:v0.0.1\n</code></pre>"},{"location":"user-guide/run-benchmark-using-docker/#building-from-source","title":"Building from Source","text":"<p>Alternatively, you can build the image locally from the Dockerfile:</p> <pre><code>docker build . -f Dockerfile -t genai-bench:dev\n</code></pre> <p>To avoid internet disruptions and network latency, it's recommended to run the benchmarking within the same network as the target inference server. You can always choose to use <code>--network host</code> if you prefer.</p> <p>To create a bridge network in docker:</p> <pre><code>docker network create benchmark-network -d bridge\n</code></pre> <p>Then, start the inference server using the standard Docker command with the additional flag <code>--network benchmark-network</code>.</p> <p>Example:</p> <pre><code>docker run -itd \\\n    --gpus \\\"device=0,1,2,3\\\" \\\n    --shm-size 10g  -v /raid/models:/models \\\n    --ulimit nofile=65535:65535   --network benchmark-network \\\n    --name sglang-v0.4.7.post1-llama4-scout-tp4 \\\n    lmsysorg/sglang:v0.4.7.post1-cu124 \\\n    python3 -m sglang.launch_server \\\n    --model-path=/models/meta-llama/Llama-4-Scout-17B-16E-Instruct \\\n    --tp 4 \\\n    --port=8080 \\\n    --host 0.0.0.0 \\\n    --context-length=131072\n</code></pre> <p>Next, start the genai-bench container with the same network flag.</p> <p>Example:</p> <p>First, create a dataset configuration file to properly specify the split:</p> <p>llava-config.json:</p> <pre><code>{\n  \"source\": {\n    \"type\": \"huggingface\",\n    \"path\": \"lmms-lab/llava-bench-in-the-wild\",\n    \"huggingface_kwargs\": {\n      \"split\": \"train\"\n    }\n  },\n  \"prompt_column\": \"question\",\n  \"image_column\": \"image\"\n}\n</code></pre> <p>Then run the benchmark with the configuration file:</p> <pre><code>docker run \\\n    -tid \\\n    --shm-size 5g \\\n    --ulimit nofile=65535:65535 \\\n    --env HF_TOKEN=\"your_HF_TOKEN\" \\\n    --network benchmark-network \\\n    -v /mnt/data/models:/models \\\n    -v $(pwd)/llava-config.json:/genai-bench/llava-config.json \\\n    --name llama-4-scout-benchmark \\\n    genai-bench:dev \\\n    benchmark \\\n    --api-backend openai \\\n    --api-base http://localhost:8080 \\\n    --api-key your_api_key \\\n    --api-model-name /models/meta-llama/Llama-4-Scout-17B-16E-Instruct \\\n    --model-tokenizer /models/meta-llama/Llama-4-Scout-17B-16E-Instruct \\\n    --task image-to-text \\\n    --max-time-per-run 10 \\\n    --max-requests-per-run 100 \\\n    --server-engine \"SGLang\" \\\n    --server-gpu-type \"H100\" \\\n    --server-version \"v0.4.7.post1\" \\\n    --server-gpu-count 4 \\\n    --traffic-scenario \"I(512,512)\" \\\n    --traffic-scenario \"I(2048,2048)\" \\\n    --num-concurrency 1 \\\n    --num-concurrency 2 \\\n    --num-concurrency 4 \\\n    --dataset-config /genai-bench/llava-config.json\n</code></pre> <p>Note that <code>genai-bench</code> is already the entrypoint of the container, so you only need to provide the command arguments afterward.</p> <p>The genai-bench runtime UI should be available through:</p> <pre><code>docker logs --follow &lt;CONTAINER_ID&gt;\n</code></pre> <p>You can also utilize <code>tmux</code> for additional parallelism and session control.</p>"},{"location":"user-guide/run-benchmark-using-docker/#monitor-benchmark-using-volume-mount","title":"Monitor benchmark using volume mount","text":"<p>To monitor benchmark interim results using the genai-bench container, you can leverage volume mounts along with the <code>--experiment-base-dir</code> option.</p> <pre><code>HOST_OUTPUT_DIR=$HOME/benchmark_results\nCONTAINER_OUTPUT_DIR=/genai-bench/benchmark_results\ndocker run \\\n    -tid \\\n    --shm-size 5g \\\n    --ulimit nofile=65535:65535 \\\n    --env HF_TOKEN=\"your_HF_TOKEN\" \\\n    --network benchmark-network \\\n    -v /mnt/data/models:/models \\\n    -v $HOST_OUTPUT_DIR:$CONTAINER_OUTPUT_DIR \\\n    -v $(pwd)/llava-config.json:/genai-bench/llava-config.json \\\n    --name llama-3.2-11b-benchmark \\\n    genai-bench:dev \\\n    benchmark \\\n    --api-backend openai \\\n    --api-base http://localhost:8080 \\\n    --api-key your_api_key \\\n    --api-model-name /models/meta-llama/Llama-4-Scout-17B-16E-Instruct \\\n    --model-tokenizer /models/meta-llama/Llama-4-Scout-17B-16E-Instruct \\\n    --task image-to-text \\\n    --max-time-per-run 10 \\\n    --max-requests-per-run 100 \\\n    --server-engine \"SGLang\" \\\n    --server-gpu-type \"H100\" \\\n    --server-version \"v0.4.7.post1\" \\\n    --server-gpu-count 4 \\\n    --traffic-scenario \"I(512,512)\" \\\n    --traffic-scenario \"I(2048,2048)\" \\\n    --num-concurrency 1 \\\n    --num-concurrency 2 \\\n    --num-concurrency 4 \\\n    --dataset-config /genai-bench/llava-config.json \\\n    --experiment-base-dir $CONTAINER_OUTPUT_DIR\n</code></pre>"},{"location":"user-guide/run-benchmark/","title":"Run Benchmark","text":"<p>Note: GenAI Bench now supports multiple cloud providers for both model endpoints and storage. For detailed multi-cloud configuration, see the Multi-Cloud Authentication &amp; Storage Guide or the Quick Reference.</p>"},{"location":"user-guide/run-benchmark/#start-a-chat-benchmark","title":"Start a chat benchmark","text":"<p>IMPORTANT: Use <code>genai-bench benchmark --help</code> to check out each command option and how to use it.</p> <p>For starter, you can try to type <code>genai-bench benchmark</code>, it will prompt the list of options you need to specify.</p> <p>Below is a sample command you can use to start a benchmark. The command will connect with a server running on address <code>http://localhost:8082</code>, using the default traffic scenario and num concurrency, and run each combination 1 minute.</p> <pre><code># Optional. This is required when you load the tokenizer from huggingface.co with a model-id\nexport HF_TOKEN=\"&lt;your-key&gt;\"\n# HF transformers will log a warning about torch not installed, since benchmark doesn't really need torch\n# and cuda, we use this env to disable the warning\nexport TRANSFORMERS_VERBOSITY=error\n\ngenai-bench benchmark --api-backend openai \\\n            --api-base \"http://localhost:8082\" \\\n            --api-key \"your-openai-api-key\" \\\n            --api-model-name \"vllm-model\" \\\n            --model-tokenizer \"/mnt/data/models/Meta-Llama-3.1-70B-Instruct\" \\\n            --task text-to-text \\\n            --max-time-per-run 15 \\\n            --max-requests-per-run 300 \\\n            --server-engine \"vLLM\" \\\n            --server-gpu-type \"H100\" \\\n            --server-version \"v0.6.0\" \\\n            --server-gpu-count 4\n</code></pre>"},{"location":"user-guide/run-benchmark/#start-a-vision-based-chat-benchmark","title":"Start a vision based chat benchmark","text":"<p>IMPORTANT: Image auto-generation pipeline is not yet implemented in this repository, hence we will be using a huggingface dataset instead.</p> <ul> <li>Image Datasets: Huggingface Llava Benchmark Images</li> </ul> <p>Below is a sample command to trigger a vision benchmark task.</p> <pre><code>genai-bench benchmark \\\n            --api-backend openai \\\n            --api-key \"your-openai-api-key\" \\\n            --api-base \"http://localhost:8180\" \\\n            --api-model-name \"/models/Phi-3-vision-128k-instruct\" \\\n            --model-tokenizer \"/models/Phi-3-vision-128k-instruct\" \\\n            --task image-to-text \\\n            --max-time-per-run 15 \\\n            --max-requests-per-run 300 \\\n            --server-engine vLLM \\\n            --server-gpu-type A100-80G \\\n            --server-version \"v0.6.0\" \\\n            --server-gpu-count 4 \\\n            --traffic-scenario \"I(256,256)\" \\\n            --traffic-scenario \"I(1024,1024)\" \\\n            --num-concurrency 1 \\\n            --num-concurrency 8 \\\n            --dataset-config ./examples/dataset_configs/config_llava-bench-in-the-wild.json\n</code></pre>"},{"location":"user-guide/run-benchmark/#start-an-embedding-benchmark","title":"Start an embedding benchmark","text":"<p>Below is a sample command to trigger an embedding benchmark task. Note: when running an embedding benchmark, it is recommended to set <code>--num-concurrency</code> to 1.</p> <pre><code>genai-bench benchmark --api-backend openai \\\n            --api-base \"http://172.18.0.3:8000\" \\\n            --api-key \"xxx\" \\\n            --api-model-name \"/models/e5-mistral-7b-instruct\" \\\n            --model-tokenizer \"/mnt/data/models/e5-mistral-7b-instruct\" \\\n            --task text-to-embeddings \\\n            --server-engine \"SGLang\" \\\n            --max-time-per-run 15 \\\n            --max-requests-per-run 1500 \\\n            --traffic-scenario \"E(64)\" \\\n            --traffic-scenario \"E(128)\" \\\n            --traffic-scenario \"E(512)\" \\\n            --traffic-scenario \"E(1024)\" \\\n            --server-gpu-type \"H100\" \\\n            --server-version \"v0.4.2\" \\\n            --server-gpu-count 1\n</code></pre>"},{"location":"user-guide/run-benchmark/#start-a-rerank-benchmark-against-oci-cohere","title":"Start a rerank benchmark against OCI Cohere","text":"<p>Below is a sample command to trigger a benchmark against cohere chat API.</p> <pre><code>genai-bench benchmark --api-backend oci-cohere \\\n            --config-file /home/ubuntu/.oci/config \\\n            --api-base \"https://ppe.inference.generativeai.us-chicago-1.oci.oraclecloud.com\" \\\n            --api-model-name \"rerank-v3.5\" \\\n            --model-tokenizer \"Cohere/rerank-v3.5\" \\\n            --server-engine \"cohere-TensorRT\" \\\n            --task text-to-rerank \\\n            --num-concurrency 1 \\\n            --server-gpu-type A100-80G \\\n            --server-version \"1.7.0\" \\\n            --server-gpu-count 4 \\\n            --max-time-per-run 15 \\\n            --max-requests-per-run 3 \\\n            --additional-request-params '{\"compartmentId\": \"COMPARTMENTID\", \"endpointId\": \"ENDPOINTID\", \"servingType\": \"DEDICATED\"}' \\\n            --num-workers 4\n</code></pre>"},{"location":"user-guide/run-benchmark/#start-a-benchmark-against-oci-cohere","title":"Start a benchmark against OCI Cohere","text":"<p>Below is a sample command to trigger a benchmark against cohere chat API.</p> <pre><code>genai-bench benchmark --api-backend oci-cohere \\\n            --config-file /home/ubuntu/.oci/config \\\n            --api-base \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\" \\\n            --api-model-name \"c4ai-command-r-08-2024\" \\\n            --model-tokenizer \"/home/ubuntu/c4ai-command-r-08-2024\" \\\n            --server-engine \"vLLM\" \\\n            --task text-to-text \\\n            --num-concurrency 1 \\\n            --server-gpu-type A100-80G \\\n            --server-version \"command_r_082024_v1_7\" \\\n            --server-gpu-count 4 \\\n            --max-time-per-run 15 \\\n            --max-requests-per-run 300 \\\n            --additional-request-params '{\"compartmentId\": \"COMPARTMENTID\", \"endpointId\": \"ENDPOINTID\", \"servingType\": \"DEDICATED\"}' \\\n            --num-workers 4\n</code></pre>"},{"location":"user-guide/run-benchmark/#monitor-a-benchmark","title":"Monitor a benchmark","text":"<p>IMPORTANT: logs in genai-bench are all useful. Please keep an eye on WARNING logs when you finish one benchmark.</p>"},{"location":"user-guide/run-benchmark/#specify-traffic-scenario-and-num-concurrency","title":"Specify --traffic-scenario and --num-concurrency","text":"<p>IMPORTANT: Please use <code>genai-bench benchmark --help</code> to check out the latest default value of <code>--num-concurrency</code> and <code>--traffic-scenario</code>.</p> <p>Both options are defined as multi-value options in click. Meaning you can pass this command multiple times. If you want to define your own <code>--num-concurrency</code> or <code>--traffic-scenario</code>, you can use</p> <pre><code>genai-bench benchmark \\\n            --api-backend openai \\\n            --task text-to-text \\\n            --max-time-per-run 10 \\\n            --max-requests-per-run 300 \\\n            --num-concurrency 1 --num-concurrency 2 --num-concurrency 4 \\\n            --num-concurrency 8 --num-concurrency 16 --num-concurrency 32 \\\n            --traffic-scenario \"N(480,240)/(300,150)\" --traffic-scenario \"D(100,100)\"\n</code></pre>"},{"location":"user-guide/run-benchmark/#notes-on-specific-options","title":"Notes on specific options","text":"<p>To manage each run or iteration in an experiment, genai-bench uses two parameters to control the exit logic. You can find more details in the <code>manage_run_time</code> function located in utils.py. Combination of <code>--max-time-per-run</code> and <code>--max-requests-per-run</code> should save overall time of one benchmark.</p> <p>For light traffic scenarios, such as D(7800,200) or lighter, we recommend the following settings:</p> <pre><code>            --max-time-per-run 10 \\\n            --max-requests-per-run 300 \\\n</code></pre> <p>For heavier traffic scenarios, like <code>D(16000,200)</code> or <code>D(128000,200)</code>, use the following configuration:</p> <pre><code>            --max-time-per-run 30 \\\n            --max-requests-per-run 100 \\\n            --traffic-scenario \"D(16000,200)\" \\\n            --traffic-scenario \"D(32000,200)\" \\\n            --traffic-scenario \"D(128000,200)\" \\\n            --num-concurrency 1 \\\n            --num-concurrency 2 \\\n            --num-concurrency 4 \\\n            --num-concurrency 8 \\\n            --num-concurrency 16 \\\n            --num-concurrency 32 \\\n</code></pre>"},{"location":"user-guide/run-benchmark/#distributed-benchmark","title":"Distributed Benchmark","text":"<p>If you see the message below in the genai-bench logs, it indicates that a single process is insufficient to generate the desired load.</p> <pre><code>CPU usage above 90%! This may constrain your throughput and may even give inconsistent response time measurements!\n</code></pre> <p>To address this, you can increase the number of worker processes using the <code>--num-workers</code> option. For example, to spin up 4 worker processes, use:</p> <pre><code>    --num-workers 4\n    --master-port 5577\n</code></pre> <p>This distributes the load across multiple processes on a single machine, improving performance and ensuring your benchmark runs smoothly.</p>"},{"location":"user-guide/run-benchmark/#notes-on-usage","title":"Notes on Usage","text":"<ol> <li>This feature is experimental, so monitor the system's behavior when enabling multiple workers.</li> <li>Recommended Limit: Do not set the number of workers to more than 16, as excessive worker processes can lead to resource contention and diminished performance.</li> <li>Ensure your system has sufficient CPU and memory resources to support the desired number of workers.</li> <li>Adjust the number of workers based on your target load and system capacity to achieve optimal results.</li> </ol>"},{"location":"user-guide/run-benchmark/#using-dataset-configurations","title":"Using Dataset Configurations","text":"<p>Genai-bench supports flexible dataset configurations through two approaches:</p>"},{"location":"user-guide/run-benchmark/#simple-cli-usage-for-basic-datasets","title":"Simple CLI Usage (for basic datasets)","text":"<pre><code># Local CSV file\n--dataset-path /path/to/data.csv \\\n--dataset-prompt-column \"prompt\"\n\n# HuggingFace dataset with simple options\n--dataset-path squad \\\n--dataset-prompt-column \"question\"\n\n# Local text file (default)\n--dataset-path /path/to/prompts.txt\n</code></pre>"},{"location":"user-guide/run-benchmark/#advanced-configuration-files-for-complex-setups","title":"Advanced Configuration Files (for complex setups)","text":"<p>For advanced HuggingFace configurations, create a JSON config file:</p> <p>Important Note for HuggingFace Datasets: When using HuggingFace datasets, you should always check if you need a <code>split</code>, <code>subset</code> parameter to avoid errors. If you don't specify, HuggingFace's <code>load_dataset</code> may return a <code>DatasetDict</code> object instead of a <code>Dataset</code>, which will cause the benchmark to fail.</p> <p>config.json: <pre><code>{\n  \"source\": {\n    \"type\": \"huggingface\",\n    \"path\": \"ccdv/govreport-summarization\",\n    \"huggingface_kwargs\": {\n      \"split\": \"train\",\n      \"revision\": \"main\",\n      \"streaming\": true\n    }\n  },\n  \"prompt_column\": \"report\"\n}\n</code></pre></p> <p>Vision dataset config: <pre><code>{\n  \"source\": {\n    \"type\": \"huggingface\",\n    \"path\": \"BLINK-Benchmark/BLINK\",\n    \"huggingface_kwargs\": {\n      \"split\": \"test\",\n      \"name\": \"Jigsaw\"\n    }\n  },\n  \"prompt_column\": \"question\",\n  \"image_column\": \"image_1\"\n}\n</code></pre></p> <p>Example for the llava-bench-in-the-wild dataset: <pre><code>{\n  \"source\": {\n    \"type\": \"huggingface\",\n    \"path\": \"lmms-lab/llava-bench-in-the-wild\",\n    \"huggingface_kwargs\": {\n      \"split\": \"train\"\n    }\n  },\n  \"prompt_column\": \"question\",\n  \"image_column\": \"image\"\n}\n</code></pre></p> <p>Then use: <code>--dataset-config config.json</code></p> <p>Benefits of config files: - Access to ALL HuggingFace <code>load_dataset</code> parameters - Reusable and version-controllable - Support for complex configurations - Future-proof (no CLI updates needed for new HuggingFace features)</p>"},{"location":"user-guide/upload-benchmark-result/","title":"Uploading Benchmark Results to Cloud Storage","text":"<p>Note: For comprehensive multi-cloud storage authentication and configuration, see the Multi-Cloud Authentication &amp; Storage Guide.</p>"},{"location":"user-guide/upload-benchmark-result/#oci-object-storage-legacy","title":"OCI Object Storage (Legacy)","text":"<p>GenAI Bench supports uploading benchmark results directly to OCI Object Storage. This feature is useful for: - Storing benchmark results in a centralized location - Sharing results with team members - Maintaining a historical record of benchmarks - Analyzing results across different runs</p> <p>To enable result uploading, use the following options with the <code>benchmark</code> command:</p> <p><pre><code>genai-bench benchmark \\\n    --api-base \"http://localhost:8082\" \\\n    --api-key \"your-openai-api-key\" \\\n    --api-model-name \"vllm-model\" \\\n    --model-tokenizer \"/mnt/data/models/Meta-Llama-3.1-70B-Instruct\" \\\n    --task text-to-text \\\n    --max-time-per-run 15 \\\n    --max-requests-per-run 300 \\\n    --server-engine \"vLLM\" \\\n    --server-gpu-type \"H100\" \\\n    --server-version \"v0.6.0\" \\\n    --server-gpu-count 4 \\\n    --upload-results \\\n    --storage-bucket \"your-bucket-name\" \\\n    --storage-provider oci\n</code></pre> By default, GenAI Bench uses OCI User Principal for authentication and authorization. The default namespace is the current tenancy, and the default region is the current region in which the client is positioned. You can override the namespace and region using the <code>--namespace</code> and <code>--region</code> options, respectively. Alternatively, you can change the authentication and authorization mechanism using the <code>--auth</code> option. The default object prefix is empty, but you can specify a prefix using the <code>--storage-prefix</code> option.</p>"},{"location":"user-guide/upload-benchmark-result/#multi-cloud-storage-support","title":"Multi-Cloud Storage Support","text":"<p>GenAI Bench now supports multiple cloud storage providers: - AWS S3: Use <code>--storage-provider aws</code> - Azure Blob Storage: Use <code>--storage-provider azure</code> - GCP Cloud Storage: Use <code>--storage-provider gcp</code> - GitHub Releases: Use <code>--storage-provider github</code></p> <p>For detailed configuration and authentication options for each provider, please refer to the Multi-Cloud Authentication &amp; Storage Guide.</p>"}]}