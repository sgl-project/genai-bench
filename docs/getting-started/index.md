# Getting Started

Welcome to GenAI Bench! This section will help you get up and running quickly.

## Overview

GenAI Bench is a powerful benchmark tool designed for comprehensive token-level performance evaluation of large language model (LLM) serving systems. It supports multiple cloud providers for both model endpoints and storage.

## Quick Links

<div class="grid cards" markdown>

-   :material-download:{ .lg .middle } **Installation**

    ---

    Get GenAI Bench installed on your system

    [:octicons-arrow-right-24: Installation Guide](installation.md)

-   :material-list-box:{ .lg .middle } **Task Types**

    ---

    Learn about supported benchmark tasks

    [:octicons-arrow-right-24: Task Definition](task-definition.md)

-   :material-console:{ .lg .middle } **CLI Usage**

    ---

    Master the command-line interface

    [:octicons-arrow-right-24: CLI Guidelines](cli-guidelines.md)

-   :material-chart-line:{ .lg .middle } **Metrics**

    ---

    Understand benchmark metrics

    [:octicons-arrow-right-24: Metrics Definition](metrics-definition.md)

</div>

## Prerequisites

Before you begin, ensure you have:

- Python 3.10-3.13
- Access to at least one supported LLM endpoint
- (Optional) Cloud storage credentials for result upload

## Next Steps

1. [Install GenAI Bench](installation.md)
2. [Configure your authentication](../user-guide/multi-cloud-auth-storage.md)
3. [Run your first benchmark](../user-guide/run-benchmark.md)