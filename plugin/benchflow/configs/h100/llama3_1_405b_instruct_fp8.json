{
  "execution_mode": "sequential",
  "max_parallel": 1,
  "workflows": [
    {
      "name": "llama-3.1-405b",
      "service": {
        "image": "vllm/vllm-openai",
        "version": "v0.6.3.post1",
        "shm_size": "15g",
        "num_gpu_devices": 8,
        "port": 8080,
        "volumes": [
          "/mnt/data/models:/models:ro"
        ],
        "env_vars": {
          "HUGGING_FACE_HUB_TOKEN": "<your-huggingface-token>"
        },
        "extra_args": [
          "--model=/models/Llama-3.1-405B-Instruct-FP8",
          "--served-model-name=vllm-model",
          "--tensor-parallel-size=8",
          "--max-model-len=131072",
          "--gpu-memory-utilization=0.95",
          "--enable-chunked-prefill"
        ]
      },
      "bench": {
        "image": "phx.ocir.io/idqj093njucb/genai-bench",
        "version": "0.1.85",
        "volumes": [
          "/mnt/data/models:/models:ro"
        ],
        "env_vars": {
          "GENAI_BENCH_LOGGING_LEVEL": "INFO",
          "HUGGINGFACE_API_KEY": "<your-huggingface-token>"
        },
        "extra_args": [
          "--api-key=your_api_key",
          "--api-model-name=vllm-model",
          "--model-tokenizer=/models/Llama-3.1-405B-Instruct-FP8",
          "--task=text-to-text",
          "--max-time-per-run=15",
          "--max-requests-per-run=300",
          "--server-engine=vLLM",
          "--server-gpu-type=H100",
          "--server-version=v0.6.3.post1",
          "--server-gpu-count=8"
        ]
      }
    }
  ]
}