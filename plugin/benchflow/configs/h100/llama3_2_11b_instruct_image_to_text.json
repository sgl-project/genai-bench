{
  "execution_mode": "parallel",
  "max_parallel": 8,
  "workflows": [
    {
      "name": "llama-3.2-11b-image-to-text",
      "service": {
        "image": "vllm/vllm-openai",
        "version": "v0.6.3.post1",
        "shm_size": "15g",
        "num_gpu_devices": 1,
        "port": 8080,
        "volumes": [
          "/mnt/data/models:/models:ro"
        ],
        "env_vars": {
          "HUGGING_FACE_HUB_TOKEN": "<your-huggingface-token>"
        },
        "extra_args": [
          "--model=/models/Llama-3.2-11B-Vision-Instruct",
          "--served-model-name=vllm-model",
          "--tensor-parallel-size=1",
          "--max-num-seqs=32",
          "--enforce-eager",
          "--preemption-mode=swap",
          "--limit-mm-per-prompt=image=1",
          "--max-model-len=131072",
          "--gpu-memory-utilization=0.99"
        ]
      },
      "bench": {
        "image": "phx.ocir.io/idqj093njucb/genai-bench",
        "version": "0.1.85",
        "volumes": [
          "/mnt/data/models:/models:ro",
          "/home/changsu/images:/genai-bench/images:rw"
        ],
        "env_vars": {
          "GENAI_BENCH_LOGGING_LEVEL": "INFO",
          "HUGGINGFACE_API_KEY": "<your-huggingface-token>"
        },
        "extra_args": [
          "--api-key=your_api_key",
          "--api-model-name=vllm-model",
          "--model-tokenizer=/models/Llama-3.2-11B-Vision-Instruct",
          "--task=image-to-text",
          "--max-time-per-run=15",
          "--max-requests-per-run=300",
          "--server-engine=vLLM",
          "--server-gpu-type=H100",
          "--server-version=v0.6.3.post1",
          "--server-gpu-count=1",
          "--traffic-scenario=I(512,512)",
          "--traffic-scenario=I(2048,2048)",
          "--dataset-path=./images/questions.jsonl"
        ]
      }
    }
  ]
}